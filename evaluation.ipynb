{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659ed735-a976-4d88-ba0f-b67a58127a45",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa6eae8-0076-4eca-8e73-704bc09423ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from glob import glob \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import torchmetrics as tm\n",
    "import torchmetrics.classification as tm_cls\n",
    "import torch\n",
    "\n",
    "from dataset import VADMelDataModule\n",
    "from inference import VADPredictor\n",
    "from utils.load_config import load_config\n",
    "from utils.get_frame_targets import get_frame_targets\n",
    "from utils.evaluating import plotly_plot_roc\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a8ae5-bace-4195-96f2-e6370319547b",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94cc9a4-ec90-4493-8260-e380ca63270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# np.random.seed(42)\n",
    "\n",
    "paths = {'sample':'data/samples/5ed8a1c0f3ea2.flac',\n",
    "        'config_name': '32_n_frames.yml',\n",
    "        'ckpt_folder': 'checkpoints/32_n_frames',\n",
    "        'weight_folder': 'weights/VADNet_13_0.1502_0.9506.pt'}\n",
    "\n",
    "glob_paths = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387439b-2cd8-4f7e-88a3-b0445d46c90b",
   "metadata": {},
   "source": [
    "Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6251f67c-8e44-4288-abd1-fdcb6024a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg, ckpt_folder = load_config('./configs/' + paths['config_name'])\n",
    "params = {key: value for key, value in cfg['data'].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b122732-fba6-48c1-a2be-ce1411f427dd",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058b08bc-8349-48fc-86f5-f935c3be184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = VADPredictor(ckpt_folder = paths['ckpt_folder'], weight_folder = paths['weight_folder'], device=device)\n",
    "mel_spec =  torchaudio.transforms.MelSpectrogram(n_fft=params['nfft'], hop_length=params['hop_length'], n_mels=params['n_mels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce606240-83cd-40de-9c46-38ed801185e6",
   "metadata": {},
   "source": [
    "### ROC - curve. AUROC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0231bb-d063-4c99-a5b9-ffffa5c8fa5c",
   "metadata": {},
   "source": [
    "For a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164f0b83-6809-455a-bc1b-8e800d7f3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, _ = torchaudio.load(paths['sample'])\n",
    "spec = torch.log(mel_spec(audio) + EPS)\n",
    "total_frames = spec.shape[-1]\n",
    "probs = torch.from_numpy(predictor.predict(audio_path = paths['sample'], threshold = None))\n",
    "label = get_frame_targets(paths['sample'], total_frames=total_frames, hop_length=params['hop_length']).squeeze(0).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c571c56b-44e4-4005-b5a7-72ab4e4973e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9997934103012085\n"
     ]
    }
   ],
   "source": [
    "# roc\n",
    "bn_roc = tm_cls.BinaryROC(thresholds = None).to(device)\n",
    "fprs_i, tprs_i, thresholds_i = bn_roc(preds = probs, target = label)\n",
    "# auc\n",
    "bn_auroc = tm_cls.BinaryAUROC(thresholds = None).to(device)\n",
    "auc_value = bn_auroc(preds = probs, target = label).item()\n",
    "print('AUC: ', auc_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa005257-02f2-472d-bf3b-d18c22ca9cb5",
   "metadata": {},
   "source": [
    "If you need plot, uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba1fd0d-9abc-441c-bb56-a0484859e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "# plotly_plot_roc(fprs_i, tprs_i, auc_value, desire_fr_val_i=0.0, desire_tr_val_i=0.99, vertical_line = False)\n",
    "# do not plot to upload to git\n",
    "# display(Image(filename='pics/roc_curve_for_sample.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3917be9-c000-4af0-a4d8-c081bc81fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "desire_fr_val_i = 0.0\n",
    "desire_tr_val_i = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea6beab-b6d1-4275-a8c1-a7663e22d1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold index for desired fpr and tpr: 227\n",
      "Threshold: 0.6504456996917725\n"
     ]
    }
   ],
   "source": [
    "threshold_index_i = np.argmin(np.abs(fprs_i - desire_fr_val_i) + np.abs(tprs_i - desire_tr_val_i))\n",
    "print('Threshold index for desired fpr and tpr:', threshold_index_i.item())\n",
    "print('Threshold:', thresholds_i[threshold_index_i].item()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b42e3-7329-4ae2-bcb8-5d60627cb333",
   "metadata": {},
   "source": [
    "**Inferencing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62cd4fc3-faab-40e9-bd12-a703aeb06401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to sample: data/samples/5ed8a1c0f3ea2.flac\n",
    "# Threshold: 0.87\n",
    "# Path to checkpoints: checkpoints/current_run\n",
    "# Path to weights: weights/VADNet_13_0.1502_0.9506.pt   \n",
    "!python inference.py data/samples/5ed8a1c0f3ea2.flac -plot -s -t 0.65 -c checkpoints/32_n_frames -w weights/VADNet_13_0.1502_0.9506.pt  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a03a6-85d4-4442-a8d5-c50b13d84164",
   "metadata": {},
   "source": [
    "If you need plot, uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095932f4-9a12-448b-b205-0e81e5ce556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename='pics/result.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb85cf6-80b5-4b7a-a6e9-0c54615891c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resets\n",
    "bn_roc.reset()\n",
    "bn_auroc.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efcc73b-a993-4526-9c06-374347e806b9",
   "metadata": {},
   "source": [
    "### For samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb313ab-32bb-41e1-ad66-9845a1ab8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = glob(os.path.join(str(params['data_dir']), '*.flac'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6dda92b-ceb3-428c-8878-63a93fa23843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 20388/20388 [02:29<00:00, 136.19it/s]\n"
     ]
    }
   ],
   "source": [
    "bn_roc = tm_cls.BinaryROC(thresholds = None).to(device)\n",
    "bn_auroc = tm_cls.BinaryAUROC(thresholds = None).to(device)\n",
    "\n",
    "for t in tqdm(samples):\n",
    "    t = t.replace('\\\\', '/')\n",
    "    audio, _ = torchaudio.load(t)\n",
    "    spec = torch.log(mel_spec(audio) + EPS)\n",
    "    total_frames = spec.shape[-1]\n",
    "    pred = torch.from_numpy(predictor.predict(audio_path = t, threshold = None))\n",
    "    target = get_frame_targets(t, total_frames=total_frames, hop_length=params['hop_length']).squeeze(0).to(torch.long)\n",
    "    with torch.no_grad():\n",
    "        bn_roc.update(preds=pred, target=target) # roc\n",
    "        bn_auroc.update(preds=pred, target=target) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3344cb48-ff3b-4249-a81e-cbf675595d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9754916429519653\n"
     ]
    }
   ],
   "source": [
    "auc_value = bn_auroc.compute().item()\n",
    "print('AUC: ', auc_value) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51813603-edd6-44cb-bcd1-834caeff07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs, tprs, thresholds = bn_roc.compute() # roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6238853-3206-43d0-bfd9-07f90890bb27",
   "metadata": {},
   "source": [
    "If you need plot, uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2638eb10-981b-45ee-aad9-d897d068b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "# plotly_plot_roc(fprs, tprs, auc_value, desire_fr_val_i=0.08, desire_tr_val_i=0.93, vertical_line=True) \n",
    "# do not plot to upload to git\n",
    "# display(Image(filename='pics/roc-curve-for-samples.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32fb16c5-5c8f-4b2b-ab1c-007bb84f6340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold index for desired fpr and tpr: 706552\n",
      "Threshold: 0.665117084980011\n"
     ]
    }
   ],
   "source": [
    "desire_fr_val = 0.08\n",
    "desire_tr_val = 0.93\n",
    "threshold_index = np.argmin(np.abs(fprs - desire_fr_val) + np.abs(tprs - desire_tr_val))\n",
    "print('Threshold index for desired fpr and tpr:', threshold_index.item())\n",
    "print('Threshold:', thresholds[threshold_index].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fd16906-49f9-4f31-ac68-6440ad5d5411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resets\n",
    "bn_roc.reset()\n",
    "bn_auroc.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb3553-d60d-4c57-a101-9ce3fc83b7a2",
   "metadata": {},
   "source": [
    "### Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39dab078-4bb7-4877-8e84-904fa5cb38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.65 \n",
    "params['n_workers'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d658dac-b8fd-4e01-966d-7b0a9fc2655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = VADMelDataModule(**params).setup(info = False)\n",
    "test_dataloader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4182159-d2ea-4fb0-a6b0-532c660b92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {m: getattr(tm_cls, m)(threshold = threshold, validate_args=False).to(device) for m in ['BinaryAccuracy', 'BinaryPrecision', 'BinaryRecall', 'BinaryF1Score']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5517862-0691-4527-92e0-6f6aa0799b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:29<00:00,  7.49s/it]\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in tqdm(test_dataloader):\n",
    "    inputs, targets = inputs.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = predictor.model(inputs).transpose(1, 2)    \n",
    "    [metrics[m].update(preds=preds, target=targets) for m in metrics.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7420f0f0-51c5-4053-8494-7ebefa98f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_metrics = {m: metrics[m].compute().item() for m in metrics.keys()}\n",
    "_ = [metrics[m].reset() for m in metrics.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "475c5055-8b0d-4547-95f8-66037a00a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryAccuracy: 0.9550 BinaryPrecision: 0.9615 BinaryRecall: 0.9444 BinaryF1Score: 0.9529 "
     ]
    }
   ],
   "source": [
    "_ = [print(f\"{m}: {computed_metrics[m]:.4f} \", end=\"\") for m in metrics.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ba802-8b14-4c26-bc9c-a9d0cdefd70b",
   "metadata": {},
   "source": [
    "### Comparing: MarbleNet vs VadNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dea1c-fd2b-4390-9619-ef5e7bfda12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
