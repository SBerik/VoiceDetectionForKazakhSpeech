{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2283e0-61e5-4253-bada-a5399206bf5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Dataset tesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3384f49f-625a-4572-8d11-e5a69ceaf1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data (80), last: [79]\n",
      "Valid data (20), last: [99]\n",
      "Test data (0), last: []\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Заданные пропорции\n",
    "train_percent = 0.8\n",
    "valid_percent = 0.2\n",
    "test_percent = 0.0  # Если нужно включить тестовые данные, измените это значение\n",
    "\n",
    "assert math.isclose(train_percent + valid_percent + test_percent, 1.0, rel_tol=1e-9), \"Sum doesnt equal to 1\" \n",
    "\n",
    "# Ваши данные\n",
    "data = [i for i in range(100)]\n",
    "N = len(data)\n",
    "\n",
    "# Расчёт количества данных для каждой части\n",
    "train_size = int(N * train_percent)\n",
    "valid_size = int(N * valid_percent)\n",
    "test_size = int(N * valid_percent)\n",
    "\n",
    "# Разделение данных\n",
    "train_data = data[:train_size]\n",
    "valid_data = data[train_size:train_size + valid_size]\n",
    "test_data = data[train_size + valid_size:]\n",
    "\n",
    "# Проверка\n",
    "print(f\"Train data ({len(train_data)}), last: {train_data[len(train_data)-1:]}\")\n",
    "print(f\"Valid data ({len(valid_data)}), last: {valid_data[len(valid_data)-1:]}\")\n",
    "print(f\"Test data ({len(test_data)}), last: {test_data[len(test_data)-1:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79858c5-0841-4ecd-87a3-2a8f2067134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import MelVADDataset\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960afe1c-91fe-4840-853d-68de422853b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dog says: gaw gaw\n"
     ]
    }
   ],
   "source": [
    "class Dog:\n",
    "    def __init__(self, *params):\n",
    "        # Здесь *params может быть использовано для передачи произвольного количества аргументов\n",
    "        self.params = params\n",
    "\n",
    "    def bark(self, word):\n",
    "        print(f\"The dog says: {word}\")\n",
    "\n",
    "\n",
    "# Пример использования\n",
    "params = ['Rex', 5, 'black']\n",
    "Dog(*params).bark(word='gaw gaw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56d450-d4dd-43ae-bc86-f9aebbbe48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = [t.replace('\\\\', '/') for t in glob('F:/ISSAI_KSC2_unpacked/vad_data_augmented/*.flac')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6d52b-47e0-4c4d-8481-d7976b01330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MelVADDataset(\n",
    "    path_list = speeches, \n",
    "    n_frames=32, \n",
    "    nfft=1024, \n",
    "    hop_length=512, \n",
    "    n_mels=128, \n",
    "    sr=16000, \n",
    "    norm=False\n",
    ")\n",
    "\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    try:\n",
    "        data = dataset[i]\n",
    "    except Exception as e:\n",
    "        print(f\"[CRITICAL ERROR] idx={i}, error={str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356692f5-2240-4f1d-b1e1-2b588554df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torchaudio\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "class GetMinNmels(th.utils.data.Dataset):  # Удален лишний символ \"__\" в названии класса\n",
    "    def __init__(self, speeches, n_fft=1024, hop_length=512, n_mels=128):  # Исправлено объявление параметров\n",
    "        self.speeches = speeches  # Список аудиофайлов\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    \n",
    "    def _get_min(self):\n",
    "        __min = float('inf')\n",
    "        for t in tqdm(self.speeches):  # Итерация по списку аудиофайлов\n",
    "            audio, _ = torchaudio.load(t)\n",
    "            spec = th.log(self.mel_spec(audio) + EPS)  # Добавление маленькой константы для предотвращения логарифма от 0\n",
    "            __min = min(__min, spec.shape[-1])  # Поиск минимальной длины спектрограммы\n",
    "        return __min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29a4b6-99e6-424c-a4b7-7253fe28181f",
   "metadata": {},
   "source": [
    "Получаем минимальный n_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616493e4-f0b3-453e-bd93-375cb7c28eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GetMinNmels(speeches, n_fft=1048, hop_length=512, n_mels=128)._get_min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571c9f7-8f16-4aab-a596-2b2db257d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c0ef5-80d5-4ec0-97a8-510348555c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches[:int(len(speeches) * 0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10eaffe-455a-4ff4-aac4-33da4feaf812",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob(os.path.join(str(speeches), '*.flac'))[:int(len(files) * 0.1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653bdb25-667a-452b-a2e1-2db5c6acaa49",
   "metadata": {},
   "source": [
    "##### Testing data shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb639f61-1ebb-44df-abfd-772974bc8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from omegaconf import OmegaConf\n",
    "from trainer import VAD\n",
    "import yaml\n",
    "import argparse\n",
    "import sys\n",
    "from dataset import VADMelDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b391d79-b94a-4819-b414-ef2823708dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем Jupyter аргументы, которые начинаются с -f\n",
    "sys.argv = sys.argv[:1]\n",
    "\n",
    "# Создаем парсер и добавляем аргумент для конфигурации\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', type=str, default='./configs/128_mels.yml', help='Path to config file')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Открываем YAML файл и загружаем конфигурацию\n",
    "cfg = yaml.load(open(args.config), Loader=yaml.FullLoader)\n",
    "\n",
    "# Или используя OmegaConf для чтения конфигурации\n",
    "# cfg = OmegaConf.load(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349837fd-f779-49d1-bf6f-4768e7c33e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_batch = {\n",
    "    \"spectro\": th.rand(8, 1, 128, 128),  # Размерность входных данных\n",
    "    \"targets\": th.randint(0, 2, (8, 128)).float()  # Бинарные метки размерности (8, 128)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54ce7e-28f6-45ea-b5c4-34b7e0094c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = VADMelDataModule(**cfg['data'])\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "\n",
    "# Size of training set: 555203\n",
    "# Size of validation set: 61689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f895c1b-ed6e-4a4b-99b7-1cbf94faf253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлечение одного батча\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Проверка содержимого батча\n",
    "spectro = batch[\"spectro\"]\n",
    "targets = batch[\"targets\"]\n",
    "\n",
    "print(f\"Размер спектрограммы: {spectro.shape}\")\n",
    "print(f\"Размер меток: {targets.shape}\")\n",
    "\n",
    "# Size of training set: 555203\n",
    "# Size of validation set: 61689\n",
    "# Размер спектрограммы: torch.Size([512, 1, 128, 32])\n",
    "# Размер меток: torch.Size([512, 1, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1d3b6-a2c0-415e-9d92-cfd2cf2a22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = VADMelDataModule(**cfg['data'])\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6a104c-de03-4768-9d30-035e4d6eca87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4276f1-3d3a-4415-9edd-1c4f63d30870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a2fd72-4c40-4a60-a8d3-b7cc49299e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from omegaconf import OmegaConf\n",
    "from trainer import VAD\n",
    "import yaml\n",
    "import argparse\n",
    "import sys\n",
    "from dataset import VADMelDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3207b9e7-0772-435a-b4fb-5e78ce497820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем Jupyter аргументы, которые начинаются с -f\n",
    "sys.argv = sys.argv[:1]\n",
    "\n",
    "# Создаем парсер и добавляем аргумент для конфигурации\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', type=str, default='./configs/128_mels.yml', help='Path to config file')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Открываем YAML файл и загружаем конфигурацию\n",
    "cfg = yaml.load(open(args.config), Loader=yaml.FullLoader)\n",
    "\n",
    "# Или используя OmegaConf для чтения конфигурации\n",
    "# cfg = OmegaConf.load(args.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc59ce81-9ad0-4273-a0a2-44da386d51b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Testing config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169f2f8d-7ce6-4cdf-9321-322d6705390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAD(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e8c18d-9681-44a6-89d8-ff0e7b6a2c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 18349\n",
      "Size of validation set: 2039\n"
     ]
    }
   ],
   "source": [
    "data_module = VADMelDataModule(**cfg['data'])\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c2db1-7b46-4841-9583-9b30c88df6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_num = 0\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    model.train()  # Переводим модель в режим тренировки\n",
    "    training_loss = model.training_step(batch, batch_idx=batch_idx)  # Передаем реальный батч\n",
    "    print(f'BATCH NUM: {batch_num}')\n",
    "    print(\"Training Loss:\", training_loss.item(), th.mean(training_loss))\n",
    "    batch_num += 1\n",
    "    if batch_num > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4591dd9-7e21-44e5-a7ed-f6741cb05953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xp_config': {'model_type': 'VAD', 'dataset': 'ISSAI_KSC2'},\n",
       " 'data': {'data_dir': 'F:/ISSAI_KSC2_unpacked/temp_vad',\n",
       "  'batch_size': 512,\n",
       "  'valid_percent': 0.9,\n",
       "  'n_frames': 32,\n",
       "  'nfft': 1048,\n",
       "  'hop_length': 512,\n",
       "  'n_mels': 128,\n",
       "  'sr': 16000,\n",
       "  'norm': False,\n",
       "  'n_workers': 16,\n",
       "  'pin_memory': True,\n",
       "  'seed': 42},\n",
       " 'model': {'n_feat': 128,\n",
       "  'cnn_channels': 32,\n",
       "  'embed_dim': 256,\n",
       "  'dff': 512,\n",
       "  'num_heads': 16},\n",
       " 'training': {'optim': 'Adam', 'lr': 0.01, 'weight_decay': 1e-05},\n",
       " 'trainer': {'fast_dev_run': False,\n",
       "  'accelerator': 'gpu',\n",
       "  'devices': 1,\n",
       "  'precision': 32,\n",
       "  'accumulate_grad_batches': 1,\n",
       "  'profiler': False,\n",
       "  'val_check_interval': 1.0,\n",
       "  'max_epochs': 100},\n",
       " 'model_checkpoint': {'monitor': 'val_loss',\n",
       "  'filename': 'VAD-{epoch:02d}',\n",
       "  'save_last': True}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd50265-79ec-4bea-ab56-451c0264f3d0",
   "metadata": {},
   "source": [
    "**Проверяем Vad** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bfa44517-1e24-41e2-94f1-279492d72c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from models import VADNet\n",
    "import omegaconf as om\n",
    "import torch.nn as nn\n",
    "import torchmetrics.classification as tm # Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f4e07f6-048b-47af-a7ac-c502c96623db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ConvBlock, CNNEmbedder\n",
    "\n",
    "class VADNetTEST(nn.Module):\n",
    "    def __init__(self, n_feat=256, cnn_channels=32, embed_dim=256, dff=512, num_heads=16):\n",
    "        print(n_feat, cnn_channels, embed_dim, dff, num_heads)\n",
    "        super().__init__()\n",
    "        self.cnn_embedder = CNNEmbedder(ch_in=1, ch_out=cnn_channels) \n",
    "        # after the framewise flattening operation we have F'xC = (n_feat/16)*cnn_channels\n",
    "        self.fc1 = nn.Linear(in_features=int((n_feat/16)*cnn_channels), out_features=embed_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True) # changed\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        # Changed\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, dff),\n",
    "            nn.Linear(dff, embed_dim),\n",
    "        )\n",
    "        self.fc2 = nn.Linear(embed_dim, 1) # changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d88c1991-a74a-42f7-958b-40bec1676b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VADTESTINIT(pl.LightningModule):\n",
    "    def __init__(self, hparams: om.DictConfig):\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        if not isinstance(hparams, om.DictConfig):\n",
    "            hparams = om.DictConfig(hparams)\n",
    "        self.hparams.update(om.OmegaConf.to_container(hparams, resolve=True))\n",
    "        print('after', self.hparams['model'])\n",
    "        # self.hparams['model']['n_feat'] = 1000 # поменял и все работает \n",
    "        self.model = VADNetTEST(**self.hparams['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c40541c2-766d-4f81-a98a-0b13c9b3615b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after {'n_feat': 128, 'cnn_channels': 32, 'embed_dim': 256, 'dff': 512, 'num_heads': 16}\n",
      "128 32 256 512 16\n"
     ]
    }
   ],
   "source": [
    "new_hparams = VADTESTINIT(cfg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5340b40-7686-46a1-819a-a7dd6df96327",
   "metadata": {},
   "source": [
    "**Проверяем VADNET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f71a2-02a2-42cf-b427-347f83100a12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Torch ligtgning training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eecb1ee6-7f78-4590-8c22-9e40a34d59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import omegaconf as om\n",
    "import torchmetrics as tm\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from models import *\n",
    "from dataset import *\n",
    "\n",
    "\n",
    "class VADTESTING(pl.LightningModule):\n",
    "    def __init__(self, hparams: om.DictConfig):\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        if not isinstance(hparams, om.DictConfig):\n",
    "            hparams = om.DictConfig(hparams)\n",
    "        self.hparams.update(om.OmegaConf.to_container(hparams, resolve=True))\n",
    "        \n",
    "        self.model = VADNet(**self.hparams['model'])\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.auroc = tm.AUROC(task=\"binary\", num_classes=1)\n",
    "        self.acc = tm.Accuracy(task=\"binary\", threshold=0.5)\n",
    "        self.f1 = tm.F1Score(task=\"binary\", threshold=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        probs = self.model(x)\n",
    "        return probs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optim_type = self.hparams.training[\"optim\"]\n",
    "        assert  optim_type in ['Adam', 'SDG']\n",
    "        \n",
    "        if self.hparams.training[\"optim\"] == 'Adam':\n",
    "            return th.optim.Adam(self.model.parameters() ,lr=self.hparams.training[\"lr\"], weight_decay=self.hparams.training[\"weight_decay\"])\n",
    "        else: \n",
    "            return th.optim.SGD(self.model.parameters() ,lr=self.hparams.training[\"lr\"], weight_decay=self.hparams.training[\"weight_decay\"])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, t = batch['spectro'], batch['targets'].squeeze(1)\n",
    "        probs = self.forward(x).squeeze(-1)\n",
    "        loss = self.loss(probs, t)\n",
    "        self.log_dict({'train_loss':th.mean(loss)}, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        x, t = batch['spectro'], batch['targets'].squeeze(1)\n",
    "        probs = self.forward(x).squeeze(-1)\n",
    "        val_loss = self.loss(probs, t)\n",
    "\n",
    "        probs = probs.squeeze(0)\n",
    "        t = t.int().squeeze(0)\n",
    "\n",
    "        # Compute metrics\n",
    "        eval_metrics = {\n",
    "            \"val_loss\": th.mean(val_loss),\n",
    "            \"auroc\": self.auroc(probs, t),\n",
    "            \"accuracy\": self.acc(probs, t),\n",
    "            \"F1\": self.f1(probs, t)\n",
    "        }\n",
    "\n",
    "        self.log_dict(eval_metrics, on_step=False, on_epoch=True)\n",
    "\n",
    "        return th.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "760158bd-86f7-4dab-a3c4-a05e53462fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 555203\n",
      "Size of validation set: 61689\n"
     ]
    }
   ],
   "source": [
    "vad_model = VADTESTING(cfg)\n",
    "test_data_module = VADMelDataModule(**cfg['data'])\n",
    "test_data_module.setup()\n",
    "test_train_loader = test_data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a274db4c-7294-40f8-bde5-61b762e552e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH NUM: 0\n",
      "Training Loss: 0.7284665703773499\n",
      "Validation Loss: 0.7013393640518188\n",
      "BATCH NUM: 1\n",
      "Training Loss: 0.7289002537727356\n",
      "Validation Loss: 0.7090765833854675\n",
      "BATCH NUM: 2\n",
      "Training Loss: 0.723651647567749\n",
      "Validation Loss: 0.7133157253265381\n",
      "BATCH NUM: 3\n",
      "Training Loss: 0.7187821865081787\n",
      "Validation Loss: 0.7178352475166321\n",
      "BATCH NUM: 4\n",
      "Training Loss: 0.7126844525337219\n",
      "Validation Loss: 0.7127168774604797\n",
      "BATCH NUM: 5\n",
      "Training Loss: 0.7315097451210022\n",
      "Validation Loss: 0.736426055431366\n",
      "BATCH NUM: 6\n",
      "Training Loss: 0.7398598790168762\n",
      "Validation Loss: 0.7419386506080627\n",
      "BATCH NUM: 7\n",
      "Training Loss: 0.7305679321289062\n",
      "Validation Loss: 0.7371056079864502\n",
      "BATCH NUM: 8\n",
      "Training Loss: 0.7095872759819031\n",
      "Validation Loss: 0.7240737080574036\n",
      "BATCH NUM: 9\n",
      "Training Loss: 0.7044501900672913\n",
      "Validation Loss: 0.7249741554260254\n",
      "BATCH NUM: 10\n",
      "Training Loss: 0.7326357364654541\n",
      "Validation Loss: 0.7413102388381958\n",
      "BATCH NUM: 11\n",
      "Training Loss: 0.7328454256057739\n",
      "Validation Loss: 0.7372945547103882\n",
      "BATCH NUM: 12\n",
      "Training Loss: 0.7215792536735535\n",
      "Validation Loss: 0.7310488820075989\n",
      "BATCH NUM: 13\n",
      "Training Loss: 0.7247008681297302\n",
      "Validation Loss: 0.7269552946090698\n",
      "BATCH NUM: 14\n",
      "Training Loss: 0.731962263584137\n",
      "Validation Loss: 0.7329933643341064\n",
      "BATCH NUM: 15\n",
      "Training Loss: 0.7227593064308167\n",
      "Validation Loss: 0.7291277647018433\n",
      "BATCH NUM: 16\n",
      "Training Loss: 0.7251538038253784\n",
      "Validation Loss: 0.7283849716186523\n",
      "BATCH NUM: 17\n",
      "Training Loss: 0.7347058057785034\n",
      "Validation Loss: 0.7334040403366089\n",
      "BATCH NUM: 18\n",
      "Training Loss: 0.7333924770355225\n",
      "Validation Loss: 0.7348716855049133\n",
      "BATCH NUM: 19\n",
      "Training Loss: 0.7287984490394592\n",
      "Validation Loss: 0.7317821979522705\n",
      "BATCH NUM: 20\n",
      "Training Loss: 0.7297096252441406\n",
      "Validation Loss: 0.7289960384368896\n",
      "BATCH NUM: 21\n",
      "Training Loss: 0.733966588973999\n",
      "Validation Loss: 0.7278514504432678\n",
      "BATCH NUM: 22\n",
      "Training Loss: 0.7352094054222107\n",
      "Validation Loss: 0.7307431101799011\n",
      "BATCH NUM: 23\n",
      "Training Loss: 0.7388642430305481\n",
      "Validation Loss: 0.7334928512573242\n",
      "BATCH NUM: 24\n",
      "Training Loss: 0.720312774181366\n",
      "Validation Loss: 0.7236528396606445\n",
      "BATCH NUM: 25\n",
      "Training Loss: 0.7251514196395874\n",
      "Validation Loss: 0.7241149544715881\n",
      "BATCH NUM: 26\n",
      "Training Loss: 0.7263233065605164\n",
      "Validation Loss: 0.7261880040168762\n",
      "BATCH NUM: 27\n",
      "Training Loss: 0.7339467406272888\n",
      "Validation Loss: 0.7317309379577637\n",
      "BATCH NUM: 28\n",
      "Training Loss: 0.7118437886238098\n",
      "Validation Loss: 0.7149577140808105\n",
      "BATCH NUM: 29\n",
      "Training Loss: 0.72184157371521\n",
      "Validation Loss: 0.721705973148346\n",
      "BATCH NUM: 30\n",
      "Training Loss: 0.7331855297088623\n",
      "Validation Loss: 0.7292903661727905\n",
      "BATCH NUM: 31\n",
      "Training Loss: 0.7227106094360352\n",
      "Validation Loss: 0.7227308750152588\n",
      "BATCH NUM: 32\n",
      "Training Loss: 0.7302880883216858\n",
      "Validation Loss: 0.7275139689445496\n",
      "BATCH NUM: 33\n",
      "Training Loss: 0.7294375896453857\n",
      "Validation Loss: 0.7279302477836609\n",
      "BATCH NUM: 34\n",
      "Training Loss: 0.734463632106781\n",
      "Validation Loss: 0.732351541519165\n",
      "BATCH NUM: 35\n",
      "Training Loss: 0.7154462337493896\n",
      "Validation Loss: 0.7147746682167053\n",
      "BATCH NUM: 36\n",
      "Training Loss: 0.7142077088356018\n",
      "Validation Loss: 0.7150562405586243\n",
      "BATCH NUM: 37\n",
      "Training Loss: 0.732050359249115\n",
      "Validation Loss: 0.7287249565124512\n",
      "BATCH NUM: 38\n",
      "Training Loss: 0.7370918989181519\n",
      "Validation Loss: 0.7353030443191528\n",
      "BATCH NUM: 39\n",
      "Training Loss: 0.7240043878555298\n",
      "Validation Loss: 0.7229726910591125\n",
      "BATCH NUM: 40\n",
      "Training Loss: 0.7308037281036377\n",
      "Validation Loss: 0.7292587757110596\n",
      "BATCH NUM: 41\n",
      "Training Loss: 0.7177993655204773\n",
      "Validation Loss: 0.7176771759986877\n",
      "BATCH NUM: 42\n",
      "Training Loss: 0.7210279703140259\n",
      "Validation Loss: 0.720495343208313\n",
      "BATCH NUM: 43\n",
      "Training Loss: 0.7291905283927917\n",
      "Validation Loss: 0.7277106642723083\n",
      "BATCH NUM: 44\n",
      "Training Loss: 0.7300666570663452\n",
      "Validation Loss: 0.7302872538566589\n",
      "BATCH NUM: 45\n",
      "Training Loss: 0.7168650031089783\n",
      "Validation Loss: 0.7174426317214966\n",
      "BATCH NUM: 46\n",
      "Training Loss: 0.7205992341041565\n",
      "Validation Loss: 0.7202656269073486\n",
      "BATCH NUM: 47\n",
      "Training Loss: 0.7171316742897034\n",
      "Validation Loss: 0.7183780670166016\n",
      "BATCH NUM: 48\n",
      "Training Loss: 0.7367143034934998\n",
      "Validation Loss: 0.7366732954978943\n",
      "BATCH NUM: 49\n",
      "Training Loss: 0.7344526648521423\n",
      "Validation Loss: 0.7339164018630981\n",
      "BATCH NUM: 50\n",
      "Training Loss: 0.7352317571640015\n",
      "Validation Loss: 0.733975887298584\n",
      "BATCH NUM: 51\n",
      "Training Loss: 0.7138166427612305\n",
      "Validation Loss: 0.7136056423187256\n",
      "BATCH NUM: 52\n",
      "Training Loss: 0.7245144844055176\n",
      "Validation Loss: 0.7246071696281433\n",
      "BATCH NUM: 53\n",
      "Training Loss: 0.7231459021568298\n",
      "Validation Loss: 0.7230679988861084\n",
      "BATCH NUM: 54\n",
      "Training Loss: 0.7172835469245911\n",
      "Validation Loss: 0.7175392508506775\n",
      "BATCH NUM: 55\n",
      "Training Loss: 0.7160440683364868\n",
      "Validation Loss: 0.716446042060852\n",
      "BATCH NUM: 56\n",
      "Training Loss: 0.7363139390945435\n",
      "Validation Loss: 0.735752284526825\n",
      "BATCH NUM: 57\n",
      "Training Loss: 0.7314274311065674\n",
      "Validation Loss: 0.7316157221794128\n",
      "BATCH NUM: 58\n",
      "Training Loss: 0.705463707447052\n",
      "Validation Loss: 0.7053545713424683\n",
      "BATCH NUM: 59\n",
      "Training Loss: 0.7271628975868225\n",
      "Validation Loss: 0.7265916466712952\n",
      "BATCH NUM: 60\n",
      "Training Loss: 0.7174396514892578\n",
      "Validation Loss: 0.7175360321998596\n",
      "BATCH NUM: 61\n",
      "Training Loss: 0.7138010263442993\n",
      "Validation Loss: 0.713260293006897\n",
      "BATCH NUM: 62\n",
      "Training Loss: 0.7268844842910767\n",
      "Validation Loss: 0.7253450155258179\n",
      "BATCH NUM: 63\n",
      "Training Loss: 0.7301167249679565\n",
      "Validation Loss: 0.7292908430099487\n",
      "BATCH NUM: 64\n",
      "Training Loss: 0.7409209609031677\n",
      "Validation Loss: 0.7396649718284607\n",
      "BATCH NUM: 65\n",
      "Training Loss: 0.7286171913146973\n",
      "Validation Loss: 0.727891206741333\n",
      "BATCH NUM: 66\n",
      "Training Loss: 0.7132406234741211\n",
      "Validation Loss: 0.7126772403717041\n",
      "BATCH NUM: 67\n",
      "Training Loss: 0.7339653372764587\n",
      "Validation Loss: 0.7354240417480469\n",
      "BATCH NUM: 68\n",
      "Training Loss: 0.727922260761261\n",
      "Validation Loss: 0.7284676432609558\n",
      "BATCH NUM: 69\n",
      "Training Loss: 0.7249988317489624\n",
      "Validation Loss: 0.7256098389625549\n",
      "BATCH NUM: 70\n",
      "Training Loss: 0.7269649505615234\n",
      "Validation Loss: 0.7274968028068542\n",
      "BATCH NUM: 71\n",
      "Training Loss: 0.7487711906433105\n",
      "Validation Loss: 0.7487843036651611\n",
      "BATCH NUM: 72\n",
      "Training Loss: 0.7346339821815491\n",
      "Validation Loss: 0.7344096302986145\n",
      "BATCH NUM: 73\n",
      "Training Loss: 0.7188077569007874\n",
      "Validation Loss: 0.7188448905944824\n",
      "BATCH NUM: 74\n",
      "Training Loss: 0.7104141712188721\n",
      "Validation Loss: 0.7097967863082886\n",
      "BATCH NUM: 75\n",
      "Training Loss: 0.7125951051712036\n",
      "Validation Loss: 0.7124634981155396\n",
      "BATCH NUM: 76\n",
      "Training Loss: 0.7143410444259644\n",
      "Validation Loss: 0.7153481841087341\n",
      "BATCH NUM: 77\n",
      "Training Loss: 0.7289155721664429\n",
      "Validation Loss: 0.7284368276596069\n",
      "BATCH NUM: 78\n",
      "Training Loss: 0.7288216352462769\n",
      "Validation Loss: 0.7287115454673767\n",
      "BATCH NUM: 79\n",
      "Training Loss: 0.7286642789840698\n",
      "Validation Loss: 0.7281401753425598\n",
      "BATCH NUM: 80\n",
      "Training Loss: 0.7309231758117676\n",
      "Validation Loss: 0.7306570410728455\n",
      "BATCH NUM: 81\n",
      "Training Loss: 0.7231871485710144\n",
      "Validation Loss: 0.7233773469924927\n",
      "BATCH NUM: 82\n",
      "Training Loss: 0.719818115234375\n",
      "Validation Loss: 0.7201176881790161\n",
      "BATCH NUM: 83\n",
      "Training Loss: 0.7308750748634338\n",
      "Validation Loss: 0.7308793067932129\n",
      "BATCH NUM: 84\n",
      "Training Loss: 0.7236851453781128\n",
      "Validation Loss: 0.7235969305038452\n",
      "BATCH NUM: 85\n",
      "Training Loss: 0.7328190803527832\n",
      "Validation Loss: 0.7313430309295654\n",
      "BATCH NUM: 86\n",
      "Training Loss: 0.7364115118980408\n",
      "Validation Loss: 0.7357945442199707\n",
      "BATCH NUM: 87\n",
      "Training Loss: 0.7178751230239868\n",
      "Validation Loss: 0.7177820205688477\n",
      "BATCH NUM: 88\n",
      "Training Loss: 0.7244911193847656\n",
      "Validation Loss: 0.7254815697669983\n",
      "BATCH NUM: 89\n",
      "Training Loss: 0.7329700589179993\n",
      "Validation Loss: 0.7324919104576111\n",
      "BATCH NUM: 90\n",
      "Training Loss: 0.7381579875946045\n",
      "Validation Loss: 0.738131046295166\n",
      "BATCH NUM: 91\n",
      "Training Loss: 0.7292287945747375\n",
      "Validation Loss: 0.7294604778289795\n",
      "BATCH NUM: 92\n",
      "Training Loss: 0.7467031478881836\n",
      "Validation Loss: 0.7454701662063599\n",
      "BATCH NUM: 93\n",
      "Training Loss: 0.7313950061798096\n",
      "Validation Loss: 0.7315618395805359\n",
      "BATCH NUM: 94\n",
      "Training Loss: 0.7213868498802185\n",
      "Validation Loss: 0.7215489149093628\n",
      "BATCH NUM: 95\n",
      "Training Loss: 0.715733528137207\n",
      "Validation Loss: 0.7157493233680725\n",
      "BATCH NUM: 96\n",
      "Training Loss: 0.7416001558303833\n",
      "Validation Loss: 0.7423928380012512\n",
      "BATCH NUM: 97\n",
      "Training Loss: 0.729877233505249\n",
      "Validation Loss: 0.7297067046165466\n",
      "BATCH NUM: 98\n",
      "Training Loss: 0.7309762239456177\n",
      "Validation Loss: 0.7309401035308838\n",
      "BATCH NUM: 99\n",
      "Training Loss: 0.7243016362190247\n",
      "Validation Loss: 0.7242892980575562\n",
      "BATCH NUM: 100\n",
      "Training Loss: 0.7192065119743347\n",
      "Validation Loss: 0.7204841375350952\n",
      "BATCH NUM: 101\n",
      "Training Loss: 0.721610963344574\n",
      "Validation Loss: 0.7213748693466187\n",
      "BATCH NUM: 102\n",
      "Training Loss: 0.7063990235328674\n",
      "Validation Loss: 0.7061730027198792\n",
      "BATCH NUM: 103\n",
      "Training Loss: 0.7402480840682983\n",
      "Validation Loss: 0.7407271862030029\n",
      "BATCH NUM: 104\n",
      "Training Loss: 0.7393077611923218\n",
      "Validation Loss: 0.7385798692703247\n",
      "BATCH NUM: 105\n",
      "Training Loss: 0.7215299010276794\n",
      "Validation Loss: 0.7215431928634644\n",
      "BATCH NUM: 106\n",
      "Training Loss: 0.7270675897598267\n",
      "Validation Loss: 0.7271010279655457\n",
      "BATCH NUM: 107\n",
      "Training Loss: 0.7195442914962769\n",
      "Validation Loss: 0.7187349796295166\n",
      "BATCH NUM: 108\n",
      "Training Loss: 0.7171393632888794\n",
      "Validation Loss: 0.7172990441322327\n",
      "BATCH NUM: 109\n",
      "Training Loss: 0.7167040705680847\n",
      "Validation Loss: 0.7176996469497681\n",
      "BATCH NUM: 110\n",
      "Training Loss: 0.7294326424598694\n",
      "Validation Loss: 0.7288862466812134\n",
      "BATCH NUM: 111\n",
      "Training Loss: 0.736342191696167\n",
      "Validation Loss: 0.7350804805755615\n",
      "BATCH NUM: 112\n",
      "Training Loss: 0.7253732681274414\n",
      "Validation Loss: 0.7253882884979248\n",
      "BATCH NUM: 113\n",
      "Training Loss: 0.7199971675872803\n",
      "Validation Loss: 0.7204280495643616\n",
      "BATCH NUM: 114\n",
      "Training Loss: 0.7343013882637024\n",
      "Validation Loss: 0.7344708442687988\n",
      "BATCH NUM: 115\n",
      "Training Loss: 0.7245333194732666\n",
      "Validation Loss: 0.7245920300483704\n",
      "BATCH NUM: 116\n",
      "Training Loss: 0.7193166017532349\n",
      "Validation Loss: 0.7193865180015564\n",
      "BATCH NUM: 117\n",
      "Training Loss: 0.7305179238319397\n",
      "Validation Loss: 0.7317259311676025\n",
      "BATCH NUM: 118\n",
      "Training Loss: 0.7427523136138916\n",
      "Validation Loss: 0.7431105375289917\n",
      "BATCH NUM: 119\n",
      "Training Loss: 0.7208031415939331\n",
      "Validation Loss: 0.7209259271621704\n",
      "BATCH NUM: 120\n",
      "Training Loss: 0.7368881106376648\n",
      "Validation Loss: 0.7356054782867432\n",
      "BATCH NUM: 121\n",
      "Training Loss: 0.7062960863113403\n",
      "Validation Loss: 0.7063285112380981\n",
      "BATCH NUM: 122\n",
      "Training Loss: 0.7246100306510925\n",
      "Validation Loss: 0.7238629460334778\n",
      "BATCH NUM: 123\n",
      "Training Loss: 0.7356721758842468\n",
      "Validation Loss: 0.7362301349639893\n",
      "BATCH NUM: 124\n",
      "Training Loss: 0.7210173010826111\n",
      "Validation Loss: 0.7203484773635864\n",
      "BATCH NUM: 125\n",
      "Training Loss: 0.7342183589935303\n",
      "Validation Loss: 0.7344126105308533\n",
      "BATCH NUM: 126\n",
      "Training Loss: 0.7468194365501404\n",
      "Validation Loss: 0.7454851865768433\n",
      "BATCH NUM: 127\n",
      "Training Loss: 0.7196498513221741\n",
      "Validation Loss: 0.7194730043411255\n",
      "BATCH NUM: 128\n",
      "Training Loss: 0.7244503498077393\n",
      "Validation Loss: 0.7242998480796814\n",
      "BATCH NUM: 129\n",
      "Training Loss: 0.7186740636825562\n",
      "Validation Loss: 0.7196434140205383\n",
      "BATCH NUM: 130\n",
      "Training Loss: 0.727592408657074\n",
      "Validation Loss: 0.7272979021072388\n",
      "BATCH NUM: 131\n",
      "Training Loss: 0.7149983048439026\n",
      "Validation Loss: 0.7150269746780396\n",
      "BATCH NUM: 132\n",
      "Training Loss: 0.7284557223320007\n",
      "Validation Loss: 0.7287669777870178\n",
      "BATCH NUM: 133\n",
      "Training Loss: 0.7317601442337036\n",
      "Validation Loss: 0.7308877110481262\n",
      "BATCH NUM: 134\n",
      "Training Loss: 0.7159123420715332\n",
      "Validation Loss: 0.7166917324066162\n",
      "BATCH NUM: 135\n",
      "Training Loss: 0.7342597246170044\n",
      "Validation Loss: 0.7346490025520325\n",
      "BATCH NUM: 136\n",
      "Training Loss: 0.7327172756195068\n",
      "Validation Loss: 0.7336733341217041\n",
      "BATCH NUM: 137\n",
      "Training Loss: 0.718293309211731\n",
      "Validation Loss: 0.7191931009292603\n",
      "BATCH NUM: 138\n",
      "Training Loss: 0.7189946174621582\n",
      "Validation Loss: 0.7184706926345825\n",
      "BATCH NUM: 139\n",
      "Training Loss: 0.7227345705032349\n",
      "Validation Loss: 0.7219769358634949\n",
      "BATCH NUM: 140\n",
      "Training Loss: 0.7329881191253662\n",
      "Validation Loss: 0.7329665422439575\n",
      "BATCH NUM: 141\n",
      "Training Loss: 0.7238609790802002\n",
      "Validation Loss: 0.7238721251487732\n",
      "BATCH NUM: 142\n",
      "Training Loss: 0.7226929664611816\n",
      "Validation Loss: 0.7225809693336487\n",
      "BATCH NUM: 143\n",
      "Training Loss: 0.7195395827293396\n",
      "Validation Loss: 0.7196130752563477\n",
      "BATCH NUM: 144\n",
      "Training Loss: 0.712090253829956\n",
      "Validation Loss: 0.7121149897575378\n",
      "BATCH NUM: 145\n",
      "Training Loss: 0.7060168981552124\n",
      "Validation Loss: 0.7055005431175232\n",
      "BATCH NUM: 146\n",
      "Training Loss: 0.7169547080993652\n",
      "Validation Loss: 0.7169899940490723\n",
      "BATCH NUM: 147\n",
      "Training Loss: 0.7218734622001648\n",
      "Validation Loss: 0.7215493321418762\n",
      "BATCH NUM: 148\n",
      "Training Loss: 0.7383983135223389\n",
      "Validation Loss: 0.7381858825683594\n",
      "BATCH NUM: 149\n",
      "Training Loss: 0.723789632320404\n",
      "Validation Loss: 0.722993791103363\n",
      "BATCH NUM: 150\n",
      "Training Loss: 0.7089611291885376\n",
      "Validation Loss: 0.7097070217132568\n",
      "BATCH NUM: 151\n",
      "Training Loss: 0.7205743789672852\n",
      "Validation Loss: 0.7205647826194763\n",
      "BATCH NUM: 152\n",
      "Training Loss: 0.730185866355896\n",
      "Validation Loss: 0.7312346696853638\n",
      "BATCH NUM: 153\n",
      "Training Loss: 0.7164899110794067\n",
      "Validation Loss: 0.7162794470787048\n",
      "BATCH NUM: 154\n",
      "Training Loss: 0.7256401777267456\n",
      "Validation Loss: 0.7251875996589661\n",
      "BATCH NUM: 155\n",
      "Training Loss: 0.7353107929229736\n",
      "Validation Loss: 0.7358313798904419\n",
      "BATCH NUM: 156\n",
      "Training Loss: 0.7415705323219299\n",
      "Validation Loss: 0.7403489351272583\n",
      "BATCH NUM: 157\n",
      "Training Loss: 0.7040350437164307\n",
      "Validation Loss: 0.7044295072555542\n",
      "BATCH NUM: 158\n",
      "Training Loss: 0.7407634854316711\n",
      "Validation Loss: 0.7400462031364441\n",
      "BATCH NUM: 159\n",
      "Training Loss: 0.7270349860191345\n",
      "Validation Loss: 0.7268096208572388\n",
      "BATCH NUM: 160\n",
      "Training Loss: 0.7218102812767029\n",
      "Validation Loss: 0.7212464809417725\n",
      "BATCH NUM: 161\n",
      "Training Loss: 0.7291796207427979\n",
      "Validation Loss: 0.7294368743896484\n",
      "BATCH NUM: 162\n",
      "Training Loss: 0.7438091039657593\n",
      "Validation Loss: 0.7445104122161865\n",
      "BATCH NUM: 163\n",
      "Training Loss: 0.7182415127754211\n",
      "Validation Loss: 0.7178888916969299\n",
      "BATCH NUM: 164\n",
      "Training Loss: 0.7227430939674377\n",
      "Validation Loss: 0.7223978042602539\n",
      "BATCH NUM: 165\n",
      "Training Loss: 0.7554933428764343\n",
      "Validation Loss: 0.7536846399307251\n",
      "BATCH NUM: 166\n",
      "Training Loss: 0.725203275680542\n",
      "Validation Loss: 0.7265564203262329\n",
      "BATCH NUM: 167\n",
      "Training Loss: 0.7259003520011902\n",
      "Validation Loss: 0.7267028093338013\n",
      "BATCH NUM: 168\n",
      "Training Loss: 0.7164279222488403\n",
      "Validation Loss: 0.7165160179138184\n",
      "BATCH NUM: 169\n",
      "Training Loss: 0.7256048917770386\n",
      "Validation Loss: 0.7263103723526001\n",
      "BATCH NUM: 170\n",
      "Training Loss: 0.7316282987594604\n",
      "Validation Loss: 0.730118453502655\n",
      "BATCH NUM: 171\n",
      "Training Loss: 0.7197525501251221\n",
      "Validation Loss: 0.7203204035758972\n",
      "BATCH NUM: 172\n",
      "Training Loss: 0.7316443920135498\n",
      "Validation Loss: 0.7315366268157959\n",
      "BATCH NUM: 173\n",
      "Training Loss: 0.7290784120559692\n",
      "Validation Loss: 0.729275643825531\n",
      "BATCH NUM: 174\n",
      "Training Loss: 0.7483981251716614\n",
      "Validation Loss: 0.7478047013282776\n",
      "BATCH NUM: 175\n",
      "Training Loss: 0.7293987274169922\n",
      "Validation Loss: 0.7293922305107117\n",
      "BATCH NUM: 176\n",
      "Training Loss: 0.7075527906417847\n",
      "Validation Loss: 0.7070550918579102\n",
      "BATCH NUM: 177\n",
      "Training Loss: 0.7322081923484802\n",
      "Validation Loss: 0.7326748967170715\n",
      "BATCH NUM: 178\n",
      "Training Loss: 0.7322221994400024\n",
      "Validation Loss: 0.7315895557403564\n",
      "BATCH NUM: 179\n",
      "Training Loss: 0.7337665557861328\n",
      "Validation Loss: 0.7339146733283997\n",
      "BATCH NUM: 180\n",
      "Training Loss: 0.7397913932800293\n",
      "Validation Loss: 0.739956259727478\n",
      "BATCH NUM: 181\n",
      "Training Loss: 0.7406250238418579\n",
      "Validation Loss: 0.7397472858428955\n",
      "BATCH NUM: 182\n",
      "Training Loss: 0.7188832759857178\n",
      "Validation Loss: 0.7189614772796631\n",
      "BATCH NUM: 183\n",
      "Training Loss: 0.7178188562393188\n",
      "Validation Loss: 0.7189867496490479\n",
      "BATCH NUM: 184\n",
      "Training Loss: 0.7511760592460632\n",
      "Validation Loss: 0.7509405016899109\n",
      "BATCH NUM: 185\n",
      "Training Loss: 0.7409080266952515\n",
      "Validation Loss: 0.7399800419807434\n",
      "BATCH NUM: 186\n",
      "Training Loss: 0.7260488271713257\n",
      "Validation Loss: 0.7262060642242432\n",
      "BATCH NUM: 187\n",
      "Training Loss: 0.7237025499343872\n",
      "Validation Loss: 0.7239881753921509\n",
      "BATCH NUM: 188\n",
      "Training Loss: 0.7260171175003052\n",
      "Validation Loss: 0.7251125574111938\n",
      "BATCH NUM: 189\n",
      "Training Loss: 0.7164549231529236\n",
      "Validation Loss: 0.7167288064956665\n",
      "BATCH NUM: 190\n",
      "Training Loss: 0.7253051996231079\n",
      "Validation Loss: 0.7255463600158691\n",
      "BATCH NUM: 191\n",
      "Training Loss: 0.7289704084396362\n",
      "Validation Loss: 0.7284237742424011\n",
      "BATCH NUM: 192\n",
      "Training Loss: 0.7214874029159546\n",
      "Validation Loss: 0.7219767570495605\n",
      "BATCH NUM: 193\n",
      "Training Loss: 0.7321496605873108\n",
      "Validation Loss: 0.7317718267440796\n",
      "BATCH NUM: 194\n",
      "Training Loss: 0.7259842753410339\n",
      "Validation Loss: 0.7266355156898499\n",
      "BATCH NUM: 195\n",
      "Training Loss: 0.7159640789031982\n",
      "Validation Loss: 0.7152673602104187\n",
      "BATCH NUM: 196\n",
      "Training Loss: 0.7333052158355713\n",
      "Validation Loss: 0.7325239181518555\n",
      "BATCH NUM: 197\n",
      "Training Loss: 0.726436972618103\n",
      "Validation Loss: 0.7270514369010925\n",
      "BATCH NUM: 198\n",
      "Training Loss: 0.7243989706039429\n",
      "Validation Loss: 0.7250534296035767\n",
      "BATCH NUM: 199\n",
      "Training Loss: 0.7363170385360718\n",
      "Validation Loss: 0.7357533574104309\n",
      "BATCH NUM: 200\n",
      "Training Loss: 0.7215579152107239\n",
      "Validation Loss: 0.7211917638778687\n",
      "BATCH NUM: 201\n",
      "Training Loss: 0.7349998950958252\n",
      "Validation Loss: 0.7348737120628357\n",
      "BATCH NUM: 202\n",
      "Training Loss: 0.7290337085723877\n",
      "Validation Loss: 0.7296433448791504\n",
      "BATCH NUM: 203\n",
      "Training Loss: 0.7438327074050903\n",
      "Validation Loss: 0.7433457374572754\n",
      "BATCH NUM: 204\n",
      "Training Loss: 0.7393448352813721\n",
      "Validation Loss: 0.7394980192184448\n",
      "BATCH NUM: 205\n",
      "Training Loss: 0.7289862632751465\n",
      "Validation Loss: 0.7285398244857788\n",
      "BATCH NUM: 206\n",
      "Training Loss: 0.7168951034545898\n",
      "Validation Loss: 0.7176249623298645\n",
      "BATCH NUM: 207\n",
      "Training Loss: 0.7313538789749146\n",
      "Validation Loss: 0.7312201261520386\n",
      "BATCH NUM: 208\n",
      "Training Loss: 0.7400085926055908\n",
      "Validation Loss: 0.7397669553756714\n",
      "BATCH NUM: 209\n",
      "Training Loss: 0.7242728471755981\n",
      "Validation Loss: 0.7245678305625916\n",
      "BATCH NUM: 210\n",
      "Training Loss: 0.7377474904060364\n",
      "Validation Loss: 0.7366631627082825\n",
      "BATCH NUM: 211\n",
      "Training Loss: 0.7349156737327576\n",
      "Validation Loss: 0.7344104647636414\n",
      "BATCH NUM: 212\n",
      "Training Loss: 0.7334930300712585\n",
      "Validation Loss: 0.7330935001373291\n",
      "BATCH NUM: 213\n",
      "Training Loss: 0.723284125328064\n",
      "Validation Loss: 0.7240689992904663\n",
      "BATCH NUM: 214\n",
      "Training Loss: 0.7235779762268066\n",
      "Validation Loss: 0.7248153686523438\n",
      "BATCH NUM: 215\n",
      "Training Loss: 0.7069772481918335\n",
      "Validation Loss: 0.7066153883934021\n",
      "BATCH NUM: 216\n",
      "Training Loss: 0.7427916526794434\n",
      "Validation Loss: 0.7427561283111572\n",
      "BATCH NUM: 217\n",
      "Training Loss: 0.7129260897636414\n",
      "Validation Loss: 0.7123529314994812\n",
      "BATCH NUM: 218\n",
      "Training Loss: 0.7267241477966309\n",
      "Validation Loss: 0.726988673210144\n",
      "BATCH NUM: 219\n",
      "Training Loss: 0.7214276194572449\n",
      "Validation Loss: 0.7223544716835022\n",
      "BATCH NUM: 220\n",
      "Training Loss: 0.7422683835029602\n",
      "Validation Loss: 0.7419406175613403\n",
      "BATCH NUM: 221\n",
      "Training Loss: 0.7406036257743835\n",
      "Validation Loss: 0.7397817969322205\n",
      "BATCH NUM: 222\n",
      "Training Loss: 0.716122031211853\n",
      "Validation Loss: 0.7154178023338318\n",
      "BATCH NUM: 223\n",
      "Training Loss: 0.7207549810409546\n",
      "Validation Loss: 0.7216634154319763\n",
      "BATCH NUM: 224\n",
      "Training Loss: 0.73226398229599\n",
      "Validation Loss: 0.7334650158882141\n",
      "BATCH NUM: 225\n",
      "Training Loss: 0.7217825055122375\n",
      "Validation Loss: 0.7218379378318787\n",
      "BATCH NUM: 226\n",
      "Training Loss: 0.7310622930526733\n",
      "Validation Loss: 0.7313477396965027\n",
      "BATCH NUM: 227\n",
      "Training Loss: 0.7163475751876831\n",
      "Validation Loss: 0.7159290909767151\n",
      "BATCH NUM: 228\n",
      "Training Loss: 0.7116355299949646\n",
      "Validation Loss: 0.7122220396995544\n",
      "BATCH NUM: 229\n",
      "Training Loss: 0.7282251119613647\n",
      "Validation Loss: 0.7284730672836304\n",
      "BATCH NUM: 230\n",
      "Training Loss: 0.7242429852485657\n",
      "Validation Loss: 0.725173830986023\n",
      "BATCH NUM: 231\n",
      "Training Loss: 0.7042243480682373\n",
      "Validation Loss: 0.7033403515815735\n",
      "BATCH NUM: 232\n",
      "Training Loss: 0.7300257682800293\n",
      "Validation Loss: 0.729725182056427\n",
      "BATCH NUM: 233\n",
      "Training Loss: 0.7331078052520752\n",
      "Validation Loss: 0.7329317331314087\n",
      "BATCH NUM: 234\n",
      "Training Loss: 0.7319295406341553\n",
      "Validation Loss: 0.7309451103210449\n",
      "BATCH NUM: 235\n",
      "Training Loss: 0.7038499116897583\n",
      "Validation Loss: 0.7042902708053589\n",
      "BATCH NUM: 236\n",
      "Training Loss: 0.727837324142456\n",
      "Validation Loss: 0.7277956008911133\n",
      "BATCH NUM: 237\n",
      "Training Loss: 0.7222040295600891\n",
      "Validation Loss: 0.7233611345291138\n",
      "BATCH NUM: 238\n",
      "Training Loss: 0.719841718673706\n",
      "Validation Loss: 0.7181282639503479\n",
      "BATCH NUM: 239\n",
      "Training Loss: 0.7112044095993042\n",
      "Validation Loss: 0.710631251335144\n",
      "BATCH NUM: 240\n",
      "Training Loss: 0.7242243885993958\n",
      "Validation Loss: 0.7233222126960754\n",
      "BATCH NUM: 241\n",
      "Training Loss: 0.7215372323989868\n",
      "Validation Loss: 0.7210809588432312\n",
      "BATCH NUM: 242\n",
      "Training Loss: 0.7130974531173706\n",
      "Validation Loss: 0.7135952115058899\n",
      "BATCH NUM: 243\n",
      "Training Loss: 0.7221464514732361\n",
      "Validation Loss: 0.72095787525177\n",
      "BATCH NUM: 244\n",
      "Training Loss: 0.7128962278366089\n",
      "Validation Loss: 0.7131431102752686\n",
      "BATCH NUM: 245\n",
      "Training Loss: 0.7382834553718567\n",
      "Validation Loss: 0.7373054623603821\n",
      "BATCH NUM: 246\n",
      "Training Loss: 0.7245428562164307\n",
      "Validation Loss: 0.7249829173088074\n",
      "BATCH NUM: 247\n",
      "Training Loss: 0.7277904748916626\n",
      "Validation Loss: 0.728643536567688\n",
      "BATCH NUM: 248\n",
      "Training Loss: 0.7147188186645508\n",
      "Validation Loss: 0.7143090963363647\n",
      "BATCH NUM: 249\n",
      "Training Loss: 0.7243108153343201\n",
      "Validation Loss: 0.7243490219116211\n",
      "BATCH NUM: 250\n",
      "Training Loss: 0.732020378112793\n",
      "Validation Loss: 0.731544554233551\n",
      "BATCH NUM: 251\n",
      "Training Loss: 0.7239593863487244\n",
      "Validation Loss: 0.724118173122406\n",
      "BATCH NUM: 252\n",
      "Training Loss: 0.7302777171134949\n",
      "Validation Loss: 0.730055570602417\n",
      "BATCH NUM: 253\n",
      "Training Loss: 0.7278707027435303\n",
      "Validation Loss: 0.7284329533576965\n",
      "BATCH NUM: 254\n",
      "Training Loss: 0.7330185770988464\n",
      "Validation Loss: 0.7340107560157776\n",
      "BATCH NUM: 255\n",
      "Training Loss: 0.7332296371459961\n",
      "Validation Loss: 0.7328669428825378\n",
      "BATCH NUM: 256\n",
      "Training Loss: 0.7142459154129028\n",
      "Validation Loss: 0.7145094871520996\n",
      "BATCH NUM: 257\n",
      "Training Loss: 0.7334437966346741\n",
      "Validation Loss: 0.734290361404419\n",
      "BATCH NUM: 258\n",
      "Training Loss: 0.7280023097991943\n",
      "Validation Loss: 0.7270091772079468\n",
      "BATCH NUM: 259\n",
      "Training Loss: 0.7256582379341125\n",
      "Validation Loss: 0.7254677414894104\n",
      "BATCH NUM: 260\n",
      "Training Loss: 0.7227575778961182\n",
      "Validation Loss: 0.7227977514266968\n",
      "BATCH NUM: 261\n",
      "Training Loss: 0.7297902703285217\n",
      "Validation Loss: 0.7300763130187988\n",
      "BATCH NUM: 262\n",
      "Training Loss: 0.7061429619789124\n",
      "Validation Loss: 0.7063610553741455\n",
      "BATCH NUM: 263\n",
      "Training Loss: 0.7172842621803284\n",
      "Validation Loss: 0.7177756428718567\n",
      "BATCH NUM: 264\n",
      "Training Loss: 0.7270824909210205\n",
      "Validation Loss: 0.7273880243301392\n",
      "BATCH NUM: 265\n",
      "Training Loss: 0.7155430316925049\n",
      "Validation Loss: 0.7161126136779785\n",
      "BATCH NUM: 266\n",
      "Training Loss: 0.7304757833480835\n",
      "Validation Loss: 0.7314805388450623\n",
      "BATCH NUM: 267\n",
      "Training Loss: 0.7258418798446655\n",
      "Validation Loss: 0.725771963596344\n",
      "BATCH NUM: 268\n",
      "Training Loss: 0.7201479077339172\n",
      "Validation Loss: 0.7207672595977783\n",
      "BATCH NUM: 269\n",
      "Training Loss: 0.7299728989601135\n",
      "Validation Loss: 0.7301599979400635\n",
      "BATCH NUM: 270\n",
      "Training Loss: 0.7324320673942566\n",
      "Validation Loss: 0.731833815574646\n",
      "BATCH NUM: 271\n",
      "Training Loss: 0.7178627252578735\n",
      "Validation Loss: 0.7175410985946655\n",
      "BATCH NUM: 272\n",
      "Training Loss: 0.7123161554336548\n",
      "Validation Loss: 0.7128610014915466\n",
      "BATCH NUM: 273\n",
      "Training Loss: 0.7297472357749939\n",
      "Validation Loss: 0.7295690178871155\n",
      "BATCH NUM: 274\n",
      "Training Loss: 0.7178945541381836\n",
      "Validation Loss: 0.7179840803146362\n",
      "BATCH NUM: 275\n",
      "Training Loss: 0.7194011807441711\n",
      "Validation Loss: 0.7199041843414307\n",
      "BATCH NUM: 276\n",
      "Training Loss: 0.7422999143600464\n",
      "Validation Loss: 0.7411287426948547\n",
      "BATCH NUM: 277\n",
      "Training Loss: 0.7191123962402344\n",
      "Validation Loss: 0.7192864418029785\n",
      "BATCH NUM: 278\n",
      "Training Loss: 0.7340754270553589\n",
      "Validation Loss: 0.7339608073234558\n",
      "BATCH NUM: 279\n",
      "Training Loss: 0.7289304733276367\n",
      "Validation Loss: 0.7280563116073608\n",
      "BATCH NUM: 280\n",
      "Training Loss: 0.7294830679893494\n",
      "Validation Loss: 0.7292588949203491\n",
      "BATCH NUM: 281\n",
      "Training Loss: 0.7121755480766296\n",
      "Validation Loss: 0.711463451385498\n",
      "BATCH NUM: 282\n",
      "Training Loss: 0.7218822836875916\n",
      "Validation Loss: 0.7219500541687012\n",
      "BATCH NUM: 283\n",
      "Training Loss: 0.7452819347381592\n",
      "Validation Loss: 0.7437021136283875\n",
      "BATCH NUM: 284\n",
      "Training Loss: 0.7234493494033813\n",
      "Validation Loss: 0.7239504456520081\n",
      "BATCH NUM: 285\n",
      "Training Loss: 0.728283166885376\n",
      "Validation Loss: 0.7288325428962708\n",
      "BATCH NUM: 286\n",
      "Training Loss: 0.7342888116836548\n",
      "Validation Loss: 0.7341797947883606\n",
      "BATCH NUM: 287\n",
      "Training Loss: 0.7298233509063721\n",
      "Validation Loss: 0.7296240329742432\n",
      "BATCH NUM: 288\n",
      "Training Loss: 0.7216181755065918\n",
      "Validation Loss: 0.7219660878181458\n",
      "BATCH NUM: 289\n",
      "Training Loss: 0.709730863571167\n",
      "Validation Loss: 0.7101367712020874\n",
      "BATCH NUM: 290\n",
      "Training Loss: 0.7160189747810364\n",
      "Validation Loss: 0.7159562706947327\n",
      "BATCH NUM: 291\n",
      "Training Loss: 0.7225251197814941\n",
      "Validation Loss: 0.7221043705940247\n",
      "BATCH NUM: 292\n",
      "Training Loss: 0.7148409485816956\n",
      "Validation Loss: 0.7144361734390259\n",
      "BATCH NUM: 293\n",
      "Training Loss: 0.733754575252533\n",
      "Validation Loss: 0.7326849699020386\n",
      "BATCH NUM: 294\n",
      "Training Loss: 0.7239235639572144\n",
      "Validation Loss: 0.7237968444824219\n",
      "BATCH NUM: 295\n",
      "Training Loss: 0.7355605959892273\n",
      "Validation Loss: 0.7353761792182922\n",
      "BATCH NUM: 296\n",
      "Training Loss: 0.7157580852508545\n",
      "Validation Loss: 0.7167326211929321\n",
      "BATCH NUM: 297\n",
      "Training Loss: 0.7140061855316162\n",
      "Validation Loss: 0.7137088775634766\n",
      "BATCH NUM: 298\n",
      "Training Loss: 0.7240982055664062\n",
      "Validation Loss: 0.7246972322463989\n",
      "BATCH NUM: 299\n",
      "Training Loss: 0.7226861119270325\n",
      "Validation Loss: 0.7227322459220886\n",
      "BATCH NUM: 300\n",
      "Training Loss: 0.7288468480110168\n",
      "Validation Loss: 0.7284275889396667\n",
      "BATCH NUM: 301\n",
      "Training Loss: 0.7132472991943359\n",
      "Validation Loss: 0.7128721475601196\n",
      "BATCH NUM: 302\n",
      "Training Loss: 0.726349949836731\n",
      "Validation Loss: 0.7263906002044678\n",
      "BATCH NUM: 303\n",
      "Training Loss: 0.7195591330528259\n",
      "Validation Loss: 0.7188411951065063\n",
      "BATCH NUM: 304\n",
      "Training Loss: 0.7129503488540649\n",
      "Validation Loss: 0.7129391431808472\n",
      "BATCH NUM: 305\n",
      "Training Loss: 0.7277395129203796\n",
      "Validation Loss: 0.7277091145515442\n",
      "BATCH NUM: 306\n",
      "Training Loss: 0.7275371551513672\n",
      "Validation Loss: 0.7279069423675537\n",
      "BATCH NUM: 307\n",
      "Training Loss: 0.7466159462928772\n",
      "Validation Loss: 0.7455098032951355\n",
      "BATCH NUM: 308\n",
      "Training Loss: 0.7323415875434875\n",
      "Validation Loss: 0.7320663332939148\n",
      "BATCH NUM: 309\n",
      "Training Loss: 0.7248963713645935\n",
      "Validation Loss: 0.7255610227584839\n",
      "BATCH NUM: 310\n",
      "Training Loss: 0.7117557525634766\n",
      "Validation Loss: 0.7119474411010742\n",
      "BATCH NUM: 311\n",
      "Training Loss: 0.7301855683326721\n",
      "Validation Loss: 0.7296009063720703\n",
      "BATCH NUM: 312\n",
      "Training Loss: 0.7290106415748596\n",
      "Validation Loss: 0.7293207049369812\n",
      "BATCH NUM: 313\n",
      "Training Loss: 0.7344887256622314\n",
      "Validation Loss: 0.7338188886642456\n",
      "BATCH NUM: 314\n",
      "Training Loss: 0.7000307440757751\n",
      "Validation Loss: 0.6994579434394836\n",
      "BATCH NUM: 315\n",
      "Training Loss: 0.711423397064209\n",
      "Validation Loss: 0.7114996314048767\n",
      "BATCH NUM: 316\n",
      "Training Loss: 0.7227895259857178\n",
      "Validation Loss: 0.7240495681762695\n",
      "BATCH NUM: 317\n",
      "Training Loss: 0.7062720060348511\n",
      "Validation Loss: 0.7068067789077759\n",
      "BATCH NUM: 318\n",
      "Training Loss: 0.7327054142951965\n",
      "Validation Loss: 0.7319303154945374\n",
      "BATCH NUM: 319\n",
      "Training Loss: 0.7337480783462524\n",
      "Validation Loss: 0.7322936058044434\n",
      "BATCH NUM: 320\n",
      "Training Loss: 0.747305154800415\n",
      "Validation Loss: 0.7463894486427307\n",
      "BATCH NUM: 321\n",
      "Training Loss: 0.7237411737442017\n",
      "Validation Loss: 0.7244399189949036\n",
      "BATCH NUM: 322\n",
      "Training Loss: 0.7285183072090149\n",
      "Validation Loss: 0.7279850244522095\n",
      "BATCH NUM: 323\n",
      "Training Loss: 0.7279689908027649\n",
      "Validation Loss: 0.7277601957321167\n",
      "BATCH NUM: 324\n",
      "Training Loss: 0.7267070412635803\n",
      "Validation Loss: 0.7276988625526428\n",
      "BATCH NUM: 325\n",
      "Training Loss: 0.7311403751373291\n",
      "Validation Loss: 0.7307172417640686\n",
      "BATCH NUM: 326\n",
      "Training Loss: 0.7150231599807739\n",
      "Validation Loss: 0.7148996591567993\n",
      "BATCH NUM: 327\n",
      "Training Loss: 0.7063228487968445\n",
      "Validation Loss: 0.7067450881004333\n",
      "BATCH NUM: 328\n",
      "Training Loss: 0.722998857498169\n",
      "Validation Loss: 0.7238795161247253\n",
      "BATCH NUM: 329\n",
      "Training Loss: 0.7306803464889526\n",
      "Validation Loss: 0.7302874326705933\n",
      "BATCH NUM: 330\n",
      "Training Loss: 0.7348423600196838\n",
      "Validation Loss: 0.7346582412719727\n",
      "BATCH NUM: 331\n",
      "Training Loss: 0.735520601272583\n",
      "Validation Loss: 0.7342925071716309\n",
      "BATCH NUM: 332\n",
      "Training Loss: 0.7040621042251587\n",
      "Validation Loss: 0.7049235701560974\n",
      "BATCH NUM: 333\n",
      "Training Loss: 0.7171573638916016\n",
      "Validation Loss: 0.7176727056503296\n",
      "BATCH NUM: 334\n",
      "Training Loss: 0.7138396501541138\n",
      "Validation Loss: 0.7129766345024109\n",
      "BATCH NUM: 335\n",
      "Training Loss: 0.7323284149169922\n",
      "Validation Loss: 0.7327899932861328\n",
      "BATCH NUM: 336\n",
      "Training Loss: 0.7398036122322083\n",
      "Validation Loss: 0.739070475101471\n",
      "BATCH NUM: 337\n",
      "Training Loss: 0.7327578067779541\n",
      "Validation Loss: 0.732026994228363\n",
      "BATCH NUM: 338\n",
      "Training Loss: 0.7234901189804077\n",
      "Validation Loss: 0.7238434553146362\n",
      "BATCH NUM: 339\n",
      "Training Loss: 0.7223901748657227\n",
      "Validation Loss: 0.7224987149238586\n",
      "BATCH NUM: 340\n",
      "Training Loss: 0.7235113978385925\n",
      "Validation Loss: 0.7242599725723267\n",
      "BATCH NUM: 341\n",
      "Training Loss: 0.7134904861450195\n",
      "Validation Loss: 0.7131553888320923\n",
      "BATCH NUM: 342\n",
      "Training Loss: 0.7443611025810242\n",
      "Validation Loss: 0.7438717484474182\n",
      "BATCH NUM: 343\n",
      "Training Loss: 0.7181939482688904\n",
      "Validation Loss: 0.718254566192627\n",
      "BATCH NUM: 344\n",
      "Training Loss: 0.7357644438743591\n",
      "Validation Loss: 0.7358850836753845\n",
      "BATCH NUM: 345\n",
      "Training Loss: 0.7335195541381836\n",
      "Validation Loss: 0.7326851487159729\n",
      "BATCH NUM: 346\n",
      "Training Loss: 0.7346394062042236\n",
      "Validation Loss: 0.734774112701416\n",
      "BATCH NUM: 347\n",
      "Training Loss: 0.7259819507598877\n",
      "Validation Loss: 0.7260524034500122\n",
      "BATCH NUM: 348\n",
      "Training Loss: 0.7234126329421997\n",
      "Validation Loss: 0.7241489887237549\n",
      "BATCH NUM: 349\n",
      "Training Loss: 0.7176852226257324\n",
      "Validation Loss: 0.7177519798278809\n",
      "BATCH NUM: 350\n",
      "Training Loss: 0.7199666500091553\n",
      "Validation Loss: 0.7201492786407471\n",
      "BATCH NUM: 351\n",
      "Training Loss: 0.7229723930358887\n",
      "Validation Loss: 0.7231934070587158\n",
      "BATCH NUM: 352\n",
      "Training Loss: 0.7326252460479736\n",
      "Validation Loss: 0.73261958360672\n",
      "BATCH NUM: 353\n",
      "Training Loss: 0.7279548645019531\n",
      "Validation Loss: 0.7288515567779541\n",
      "BATCH NUM: 354\n",
      "Training Loss: 0.7257236242294312\n",
      "Validation Loss: 0.7254165410995483\n",
      "BATCH NUM: 355\n",
      "Training Loss: 0.7388047575950623\n",
      "Validation Loss: 0.7382715940475464\n",
      "BATCH NUM: 356\n",
      "Training Loss: 0.7345504760742188\n",
      "Validation Loss: 0.734171986579895\n",
      "BATCH NUM: 357\n",
      "Training Loss: 0.7320168018341064\n",
      "Validation Loss: 0.7317337989807129\n",
      "BATCH NUM: 358\n",
      "Training Loss: 0.723239541053772\n",
      "Validation Loss: 0.7227317094802856\n",
      "BATCH NUM: 359\n",
      "Training Loss: 0.740973711013794\n",
      "Validation Loss: 0.740302562713623\n",
      "BATCH NUM: 360\n",
      "Training Loss: 0.7325816750526428\n",
      "Validation Loss: 0.733518123626709\n",
      "BATCH NUM: 361\n",
      "Training Loss: 0.7208482027053833\n",
      "Validation Loss: 0.7209632396697998\n",
      "BATCH NUM: 362\n",
      "Training Loss: 0.7172075510025024\n",
      "Validation Loss: 0.7171283960342407\n",
      "BATCH NUM: 363\n",
      "Training Loss: 0.7231904864311218\n",
      "Validation Loss: 0.7243884205818176\n",
      "BATCH NUM: 364\n",
      "Training Loss: 0.7309551239013672\n",
      "Validation Loss: 0.731063187122345\n",
      "BATCH NUM: 365\n",
      "Training Loss: 0.7291126251220703\n",
      "Validation Loss: 0.7289113998413086\n",
      "BATCH NUM: 366\n",
      "Training Loss: 0.7285847663879395\n",
      "Validation Loss: 0.7294350266456604\n",
      "BATCH NUM: 367\n",
      "Training Loss: 0.7295473217964172\n",
      "Validation Loss: 0.7301470637321472\n",
      "BATCH NUM: 368\n",
      "Training Loss: 0.7145851254463196\n",
      "Validation Loss: 0.7138479351997375\n",
      "BATCH NUM: 369\n",
      "Training Loss: 0.7287857532501221\n",
      "Validation Loss: 0.7284877300262451\n",
      "BATCH NUM: 370\n",
      "Training Loss: 0.7279336452484131\n",
      "Validation Loss: 0.7278861999511719\n",
      "BATCH NUM: 371\n",
      "Training Loss: 0.730048418045044\n",
      "Validation Loss: 0.7299532294273376\n",
      "BATCH NUM: 372\n",
      "Training Loss: 0.7216244339942932\n",
      "Validation Loss: 0.7214511632919312\n",
      "BATCH NUM: 373\n",
      "Training Loss: 0.7282108664512634\n",
      "Validation Loss: 0.7283768653869629\n",
      "BATCH NUM: 374\n",
      "Training Loss: 0.7371772527694702\n",
      "Validation Loss: 0.7375141382217407\n",
      "BATCH NUM: 375\n",
      "Training Loss: 0.7285508513450623\n",
      "Validation Loss: 0.7285547852516174\n",
      "BATCH NUM: 376\n",
      "Training Loss: 0.7196025252342224\n",
      "Validation Loss: 0.719783365726471\n",
      "BATCH NUM: 377\n",
      "Training Loss: 0.7219812273979187\n",
      "Validation Loss: 0.7216439843177795\n",
      "BATCH NUM: 378\n",
      "Training Loss: 0.7279060482978821\n",
      "Validation Loss: 0.7284185290336609\n",
      "BATCH NUM: 379\n",
      "Training Loss: 0.7220702767372131\n",
      "Validation Loss: 0.7215172052383423\n",
      "BATCH NUM: 380\n",
      "Training Loss: 0.7126530408859253\n",
      "Validation Loss: 0.7128327488899231\n",
      "BATCH NUM: 381\n",
      "Training Loss: 0.7296702265739441\n",
      "Validation Loss: 0.7296311259269714\n",
      "BATCH NUM: 382\n",
      "Training Loss: 0.7259821891784668\n",
      "Validation Loss: 0.7265598773956299\n",
      "BATCH NUM: 383\n",
      "Training Loss: 0.7338971495628357\n",
      "Validation Loss: 0.7337269186973572\n",
      "BATCH NUM: 384\n",
      "Training Loss: 0.7266130447387695\n",
      "Validation Loss: 0.7278933525085449\n",
      "BATCH NUM: 385\n",
      "Training Loss: 0.7442946434020996\n",
      "Validation Loss: 0.7445487976074219\n",
      "BATCH NUM: 386\n",
      "Training Loss: 0.7224070429801941\n",
      "Validation Loss: 0.722078263759613\n",
      "BATCH NUM: 387\n",
      "Training Loss: 0.7169560790061951\n",
      "Validation Loss: 0.7164636254310608\n",
      "BATCH NUM: 388\n",
      "Training Loss: 0.7196727395057678\n",
      "Validation Loss: 0.7197766900062561\n",
      "BATCH NUM: 389\n",
      "Training Loss: 0.7364904284477234\n",
      "Validation Loss: 0.735283613204956\n",
      "BATCH NUM: 390\n",
      "Training Loss: 0.7281253337860107\n",
      "Validation Loss: 0.7289078235626221\n",
      "BATCH NUM: 391\n",
      "Training Loss: 0.7212697267532349\n",
      "Validation Loss: 0.7214645147323608\n",
      "BATCH NUM: 392\n",
      "Training Loss: 0.71050626039505\n",
      "Validation Loss: 0.7104270458221436\n",
      "BATCH NUM: 393\n",
      "Training Loss: 0.7167075276374817\n",
      "Validation Loss: 0.7167266011238098\n",
      "BATCH NUM: 394\n",
      "Training Loss: 0.729726254940033\n",
      "Validation Loss: 0.730676531791687\n",
      "BATCH NUM: 395\n",
      "Training Loss: 0.7135764360427856\n",
      "Validation Loss: 0.713526725769043\n",
      "BATCH NUM: 396\n",
      "Training Loss: 0.7199571132659912\n",
      "Validation Loss: 0.7205138802528381\n",
      "BATCH NUM: 397\n",
      "Training Loss: 0.7284445762634277\n",
      "Validation Loss: 0.727782666683197\n",
      "BATCH NUM: 398\n",
      "Training Loss: 0.7318830490112305\n",
      "Validation Loss: 0.7312528491020203\n",
      "BATCH NUM: 399\n",
      "Training Loss: 0.7124714851379395\n",
      "Validation Loss: 0.7120169401168823\n",
      "BATCH NUM: 400\n",
      "Training Loss: 0.7327703833580017\n",
      "Validation Loss: 0.7319867610931396\n",
      "BATCH NUM: 401\n",
      "Training Loss: 0.7237361669540405\n",
      "Validation Loss: 0.7243767380714417\n",
      "BATCH NUM: 402\n",
      "Training Loss: 0.7349282503128052\n",
      "Validation Loss: 0.7345258593559265\n",
      "BATCH NUM: 403\n",
      "Training Loss: 0.728218674659729\n",
      "Validation Loss: 0.7279654741287231\n",
      "BATCH NUM: 404\n",
      "Training Loss: 0.7261079549789429\n",
      "Validation Loss: 0.7259213924407959\n",
      "BATCH NUM: 405\n",
      "Training Loss: 0.7205798625946045\n",
      "Validation Loss: 0.7206347584724426\n",
      "BATCH NUM: 406\n",
      "Training Loss: 0.736939013004303\n",
      "Validation Loss: 0.7368007302284241\n",
      "BATCH NUM: 407\n",
      "Training Loss: 0.7330297827720642\n",
      "Validation Loss: 0.7335158586502075\n",
      "BATCH NUM: 408\n",
      "Training Loss: 0.7141776084899902\n",
      "Validation Loss: 0.7145107388496399\n",
      "BATCH NUM: 409\n",
      "Training Loss: 0.7394431233406067\n",
      "Validation Loss: 0.7390437126159668\n",
      "BATCH NUM: 410\n",
      "Training Loss: 0.7113604545593262\n",
      "Validation Loss: 0.7120265960693359\n",
      "BATCH NUM: 411\n",
      "Training Loss: 0.7221041321754456\n",
      "Validation Loss: 0.7215901613235474\n",
      "BATCH NUM: 412\n",
      "Training Loss: 0.7283970713615417\n",
      "Validation Loss: 0.7289137840270996\n",
      "BATCH NUM: 413\n",
      "Training Loss: 0.7351299524307251\n",
      "Validation Loss: 0.7344463467597961\n",
      "BATCH NUM: 414\n",
      "Training Loss: 0.7224122285842896\n",
      "Validation Loss: 0.7215127944946289\n",
      "BATCH NUM: 415\n",
      "Training Loss: 0.7355229258537292\n",
      "Validation Loss: 0.7348324060440063\n",
      "BATCH NUM: 416\n",
      "Training Loss: 0.7262783646583557\n",
      "Validation Loss: 0.7276380658149719\n",
      "BATCH NUM: 417\n",
      "Training Loss: 0.722175657749176\n",
      "Validation Loss: 0.7221379280090332\n",
      "BATCH NUM: 418\n",
      "Training Loss: 0.7288324236869812\n",
      "Validation Loss: 0.72880619764328\n",
      "BATCH NUM: 419\n",
      "Training Loss: 0.7286227941513062\n",
      "Validation Loss: 0.728786051273346\n",
      "BATCH NUM: 420\n",
      "Training Loss: 0.7372012734413147\n",
      "Validation Loss: 0.7379056215286255\n",
      "BATCH NUM: 421\n",
      "Training Loss: 0.728692889213562\n",
      "Validation Loss: 0.7285869121551514\n",
      "BATCH NUM: 422\n",
      "Training Loss: 0.7229146957397461\n",
      "Validation Loss: 0.7227026224136353\n",
      "BATCH NUM: 423\n",
      "Training Loss: 0.7277648448944092\n",
      "Validation Loss: 0.7280333638191223\n",
      "BATCH NUM: 424\n",
      "Training Loss: 0.7170431017875671\n",
      "Validation Loss: 0.7161137461662292\n",
      "BATCH NUM: 425\n",
      "Training Loss: 0.7316192984580994\n",
      "Validation Loss: 0.7313316464424133\n",
      "BATCH NUM: 426\n",
      "Training Loss: 0.7258594036102295\n",
      "Validation Loss: 0.7251192927360535\n",
      "BATCH NUM: 427\n",
      "Training Loss: 0.7287010550498962\n",
      "Validation Loss: 0.7289912104606628\n",
      "BATCH NUM: 428\n",
      "Training Loss: 0.7369450330734253\n",
      "Validation Loss: 0.7365694046020508\n",
      "BATCH NUM: 429\n",
      "Training Loss: 0.7327446341514587\n",
      "Validation Loss: 0.7328774929046631\n",
      "BATCH NUM: 430\n",
      "Training Loss: 0.7110053896903992\n",
      "Validation Loss: 0.7105298042297363\n",
      "BATCH NUM: 431\n",
      "Training Loss: 0.7282909154891968\n",
      "Validation Loss: 0.7277725338935852\n",
      "BATCH NUM: 432\n",
      "Training Loss: 0.7206957340240479\n",
      "Validation Loss: 0.71983802318573\n",
      "BATCH NUM: 433\n",
      "Training Loss: 0.7267732620239258\n",
      "Validation Loss: 0.7274807095527649\n",
      "BATCH NUM: 434\n",
      "Training Loss: 0.7368901968002319\n",
      "Validation Loss: 0.7370266914367676\n",
      "BATCH NUM: 435\n",
      "Training Loss: 0.733505368232727\n",
      "Validation Loss: 0.7344063520431519\n",
      "BATCH NUM: 436\n",
      "Training Loss: 0.7309646010398865\n",
      "Validation Loss: 0.7309830784797668\n",
      "BATCH NUM: 437\n",
      "Training Loss: 0.7430788278579712\n",
      "Validation Loss: 0.7426613569259644\n",
      "BATCH NUM: 438\n",
      "Training Loss: 0.7251688838005066\n",
      "Validation Loss: 0.7266906499862671\n",
      "BATCH NUM: 439\n",
      "Training Loss: 0.7289906740188599\n",
      "Validation Loss: 0.7301655411720276\n",
      "BATCH NUM: 440\n",
      "Training Loss: 0.7406704425811768\n",
      "Validation Loss: 0.7390550971031189\n",
      "BATCH NUM: 441\n",
      "Training Loss: 0.7018653154373169\n",
      "Validation Loss: 0.7022997140884399\n",
      "BATCH NUM: 442\n",
      "Training Loss: 0.7243661284446716\n",
      "Validation Loss: 0.7241941690444946\n",
      "BATCH NUM: 443\n",
      "Training Loss: 0.7448822259902954\n",
      "Validation Loss: 0.7438887357711792\n",
      "BATCH NUM: 444\n",
      "Training Loss: 0.7303338050842285\n",
      "Validation Loss: 0.730629563331604\n",
      "BATCH NUM: 445\n",
      "Training Loss: 0.7336585521697998\n",
      "Validation Loss: 0.7331559658050537\n",
      "BATCH NUM: 446\n",
      "Training Loss: 0.7072378396987915\n",
      "Validation Loss: 0.7069698572158813\n",
      "BATCH NUM: 447\n",
      "Training Loss: 0.7177842855453491\n",
      "Validation Loss: 0.7178171873092651\n",
      "BATCH NUM: 448\n",
      "Training Loss: 0.7195277810096741\n",
      "Validation Loss: 0.7210429906845093\n",
      "BATCH NUM: 449\n",
      "Training Loss: 0.7245919704437256\n",
      "Validation Loss: 0.7234405875205994\n",
      "BATCH NUM: 450\n",
      "Training Loss: 0.7282307744026184\n",
      "Validation Loss: 0.7285472750663757\n",
      "BATCH NUM: 451\n",
      "Training Loss: 0.728249192237854\n",
      "Validation Loss: 0.7282513380050659\n",
      "BATCH NUM: 452\n",
      "Training Loss: 0.7165051698684692\n",
      "Validation Loss: 0.7169221639633179\n",
      "BATCH NUM: 453\n",
      "Training Loss: 0.7332516312599182\n",
      "Validation Loss: 0.7336148619651794\n",
      "BATCH NUM: 454\n",
      "Training Loss: 0.727770209312439\n",
      "Validation Loss: 0.7273145318031311\n",
      "BATCH NUM: 455\n",
      "Training Loss: 0.7216563820838928\n",
      "Validation Loss: 0.7218390107154846\n",
      "BATCH NUM: 456\n",
      "Training Loss: 0.7411175966262817\n",
      "Validation Loss: 0.7400053143501282\n",
      "BATCH NUM: 457\n",
      "Training Loss: 0.7317622303962708\n",
      "Validation Loss: 0.7310391664505005\n",
      "BATCH NUM: 458\n",
      "Training Loss: 0.7046077251434326\n",
      "Validation Loss: 0.7042790055274963\n",
      "BATCH NUM: 459\n",
      "Training Loss: 0.7480478286743164\n",
      "Validation Loss: 0.7482724785804749\n",
      "BATCH NUM: 460\n",
      "Training Loss: 0.7308623790740967\n",
      "Validation Loss: 0.7309033870697021\n",
      "BATCH NUM: 461\n",
      "Training Loss: 0.7381193041801453\n",
      "Validation Loss: 0.7379551529884338\n",
      "BATCH NUM: 462\n",
      "Training Loss: 0.7177734375\n",
      "Validation Loss: 0.7184966802597046\n",
      "BATCH NUM: 463\n",
      "Training Loss: 0.7245882749557495\n",
      "Validation Loss: 0.7257784605026245\n",
      "BATCH NUM: 464\n",
      "Training Loss: 0.7376013398170471\n",
      "Validation Loss: 0.7372341752052307\n",
      "BATCH NUM: 465\n",
      "Training Loss: 0.7279394865036011\n",
      "Validation Loss: 0.7273918390274048\n",
      "BATCH NUM: 466\n",
      "Training Loss: 0.7229833602905273\n",
      "Validation Loss: 0.7241067886352539\n",
      "BATCH NUM: 467\n",
      "Training Loss: 0.7517479062080383\n",
      "Validation Loss: 0.751076877117157\n",
      "BATCH NUM: 468\n",
      "Training Loss: 0.7385984063148499\n",
      "Validation Loss: 0.7374118566513062\n",
      "BATCH NUM: 469\n",
      "Training Loss: 0.7224728465080261\n",
      "Validation Loss: 0.7227065563201904\n",
      "BATCH NUM: 470\n",
      "Training Loss: 0.7277607321739197\n",
      "Validation Loss: 0.7259464859962463\n",
      "BATCH NUM: 471\n",
      "Training Loss: 0.7316794991493225\n",
      "Validation Loss: 0.7330307960510254\n",
      "BATCH NUM: 472\n",
      "Training Loss: 0.735957682132721\n",
      "Validation Loss: 0.7339464426040649\n",
      "BATCH NUM: 473\n",
      "Training Loss: 0.7193842530250549\n",
      "Validation Loss: 0.7188173532485962\n",
      "BATCH NUM: 474\n",
      "Training Loss: 0.725507915019989\n",
      "Validation Loss: 0.726593017578125\n",
      "BATCH NUM: 475\n",
      "Training Loss: 0.7257224321365356\n",
      "Validation Loss: 0.7262916564941406\n",
      "BATCH NUM: 476\n",
      "Training Loss: 0.723482072353363\n",
      "Validation Loss: 0.7238538861274719\n",
      "BATCH NUM: 477\n",
      "Training Loss: 0.7209575176239014\n",
      "Validation Loss: 0.7215101718902588\n",
      "BATCH NUM: 478\n",
      "Training Loss: 0.7309805154800415\n",
      "Validation Loss: 0.730344295501709\n",
      "BATCH NUM: 479\n",
      "Training Loss: 0.7245845794677734\n",
      "Validation Loss: 0.7245404720306396\n",
      "BATCH NUM: 480\n",
      "Training Loss: 0.736574649810791\n",
      "Validation Loss: 0.7363608479499817\n",
      "BATCH NUM: 481\n",
      "Training Loss: 0.7343726754188538\n",
      "Validation Loss: 0.7347047924995422\n",
      "BATCH NUM: 482\n",
      "Training Loss: 0.7276062369346619\n",
      "Validation Loss: 0.7273404598236084\n",
      "BATCH NUM: 483\n",
      "Training Loss: 0.7261248230934143\n",
      "Validation Loss: 0.7268452644348145\n",
      "BATCH NUM: 484\n",
      "Training Loss: 0.7094652056694031\n",
      "Validation Loss: 0.7098300457000732\n",
      "BATCH NUM: 485\n",
      "Training Loss: 0.7216938138008118\n",
      "Validation Loss: 0.7213157415390015\n",
      "BATCH NUM: 486\n",
      "Training Loss: 0.7235231995582581\n",
      "Validation Loss: 0.723182201385498\n",
      "BATCH NUM: 487\n",
      "Training Loss: 0.7107739448547363\n",
      "Validation Loss: 0.7105321288108826\n",
      "BATCH NUM: 488\n",
      "Training Loss: 0.7337061762809753\n",
      "Validation Loss: 0.7325016856193542\n",
      "BATCH NUM: 489\n",
      "Training Loss: 0.7160335779190063\n",
      "Validation Loss: 0.7163674831390381\n",
      "BATCH NUM: 490\n",
      "Training Loss: 0.7091735005378723\n",
      "Validation Loss: 0.7094138264656067\n",
      "BATCH NUM: 491\n",
      "Training Loss: 0.7167515754699707\n",
      "Validation Loss: 0.7169337272644043\n",
      "BATCH NUM: 492\n",
      "Training Loss: 0.7260845899581909\n",
      "Validation Loss: 0.7263504862785339\n",
      "BATCH NUM: 493\n",
      "Training Loss: 0.7274934649467468\n",
      "Validation Loss: 0.727375864982605\n",
      "BATCH NUM: 494\n",
      "Training Loss: 0.7203044295310974\n",
      "Validation Loss: 0.7201244831085205\n",
      "BATCH NUM: 495\n",
      "Training Loss: 0.7250518798828125\n",
      "Validation Loss: 0.7257819771766663\n",
      "BATCH NUM: 496\n",
      "Training Loss: 0.7417989373207092\n",
      "Validation Loss: 0.7418603897094727\n",
      "BATCH NUM: 497\n",
      "Training Loss: 0.7020706534385681\n",
      "Validation Loss: 0.7020161747932434\n",
      "BATCH NUM: 498\n",
      "Training Loss: 0.7436503767967224\n",
      "Validation Loss: 0.7424050569534302\n",
      "BATCH NUM: 499\n",
      "Training Loss: 0.7288529276847839\n",
      "Validation Loss: 0.7292197942733765\n",
      "BATCH NUM: 500\n",
      "Training Loss: 0.7196874618530273\n",
      "Validation Loss: 0.7195907831192017\n",
      "BATCH NUM: 501\n",
      "Training Loss: 0.7341852188110352\n",
      "Validation Loss: 0.7330398559570312\n",
      "BATCH NUM: 502\n",
      "Training Loss: 0.7347828149795532\n",
      "Validation Loss: 0.734606146812439\n",
      "BATCH NUM: 503\n",
      "Training Loss: 0.7112886905670166\n",
      "Validation Loss: 0.711676836013794\n",
      "BATCH NUM: 504\n",
      "Training Loss: 0.7439817190170288\n",
      "Validation Loss: 0.7430509328842163\n",
      "BATCH NUM: 505\n",
      "Training Loss: 0.7293152809143066\n",
      "Validation Loss: 0.7302146553993225\n",
      "BATCH NUM: 506\n",
      "Training Loss: 0.7457548975944519\n",
      "Validation Loss: 0.7450820207595825\n",
      "BATCH NUM: 507\n",
      "Training Loss: 0.7293832302093506\n",
      "Validation Loss: 0.7300453186035156\n",
      "BATCH NUM: 508\n",
      "Training Loss: 0.7232669591903687\n",
      "Validation Loss: 0.7231029272079468\n",
      "BATCH NUM: 509\n",
      "Training Loss: 0.7298507690429688\n",
      "Validation Loss: 0.7288665771484375\n",
      "BATCH NUM: 510\n",
      "Training Loss: 0.7438818216323853\n",
      "Validation Loss: 0.7441297769546509\n",
      "BATCH NUM: 511\n",
      "Training Loss: 0.7262978553771973\n",
      "Validation Loss: 0.7266462445259094\n",
      "BATCH NUM: 512\n",
      "Training Loss: 0.7290352582931519\n",
      "Validation Loss: 0.7284635305404663\n",
      "BATCH NUM: 513\n",
      "Training Loss: 0.7336682081222534\n",
      "Validation Loss: 0.7336476445198059\n",
      "BATCH NUM: 514\n",
      "Training Loss: 0.7286531329154968\n",
      "Validation Loss: 0.7287484407424927\n",
      "BATCH NUM: 515\n",
      "Training Loss: 0.7050725817680359\n",
      "Validation Loss: 0.7057822942733765\n",
      "BATCH NUM: 516\n",
      "Training Loss: 0.7218125462532043\n",
      "Validation Loss: 0.7222368717193604\n",
      "BATCH NUM: 517\n",
      "Training Loss: 0.7386701107025146\n",
      "Validation Loss: 0.7386501431465149\n",
      "BATCH NUM: 518\n",
      "Training Loss: 0.7283221483230591\n",
      "Validation Loss: 0.7275606989860535\n",
      "BATCH NUM: 519\n",
      "Training Loss: 0.7116956114768982\n",
      "Validation Loss: 0.7123222351074219\n",
      "BATCH NUM: 520\n",
      "Training Loss: 0.7438772320747375\n",
      "Validation Loss: 0.7428744435310364\n",
      "BATCH NUM: 521\n",
      "Training Loss: 0.7170224189758301\n",
      "Validation Loss: 0.7164020538330078\n",
      "BATCH NUM: 522\n",
      "Training Loss: 0.7333224415779114\n",
      "Validation Loss: 0.7321048974990845\n",
      "BATCH NUM: 523\n",
      "Training Loss: 0.7385637164115906\n",
      "Validation Loss: 0.740097165107727\n",
      "BATCH NUM: 524\n",
      "Training Loss: 0.7342371940612793\n",
      "Validation Loss: 0.7334575653076172\n",
      "BATCH NUM: 525\n",
      "Training Loss: 0.7461546063423157\n",
      "Validation Loss: 0.7462591528892517\n",
      "BATCH NUM: 526\n",
      "Training Loss: 0.7250160574913025\n",
      "Validation Loss: 0.7243569493293762\n",
      "BATCH NUM: 527\n",
      "Training Loss: 0.7160301208496094\n",
      "Validation Loss: 0.71561598777771\n",
      "BATCH NUM: 528\n",
      "Training Loss: 0.741933286190033\n",
      "Validation Loss: 0.7422881722450256\n",
      "BATCH NUM: 529\n",
      "Training Loss: 0.7344390153884888\n",
      "Validation Loss: 0.7331874966621399\n",
      "BATCH NUM: 530\n",
      "Training Loss: 0.730211615562439\n",
      "Validation Loss: 0.7306585311889648\n",
      "BATCH NUM: 531\n",
      "Training Loss: 0.7286434173583984\n",
      "Validation Loss: 0.7287202477455139\n",
      "BATCH NUM: 532\n",
      "Training Loss: 0.731522798538208\n",
      "Validation Loss: 0.7313202619552612\n",
      "BATCH NUM: 533\n",
      "Training Loss: 0.7388660907745361\n",
      "Validation Loss: 0.7385596632957458\n",
      "BATCH NUM: 534\n",
      "Training Loss: 0.7334401607513428\n",
      "Validation Loss: 0.7327710390090942\n",
      "BATCH NUM: 535\n",
      "Training Loss: 0.7157562375068665\n",
      "Validation Loss: 0.7157775163650513\n",
      "BATCH NUM: 536\n",
      "Training Loss: 0.719123363494873\n",
      "Validation Loss: 0.7200195789337158\n",
      "BATCH NUM: 537\n",
      "Training Loss: 0.7089612483978271\n",
      "Validation Loss: 0.7092399597167969\n",
      "BATCH NUM: 538\n",
      "Training Loss: 0.7410477995872498\n",
      "Validation Loss: 0.7410387396812439\n",
      "BATCH NUM: 539\n",
      "Training Loss: 0.7430859208106995\n",
      "Validation Loss: 0.7434239983558655\n",
      "BATCH NUM: 540\n",
      "Training Loss: 0.7300939559936523\n",
      "Validation Loss: 0.730445384979248\n",
      "BATCH NUM: 541\n",
      "Training Loss: 0.7064637541770935\n",
      "Validation Loss: 0.706929087638855\n",
      "BATCH NUM: 542\n",
      "Training Loss: 0.7277882099151611\n",
      "Validation Loss: 0.7276822328567505\n",
      "BATCH NUM: 543\n",
      "Training Loss: 0.7345066070556641\n",
      "Validation Loss: 0.7342408895492554\n",
      "BATCH NUM: 544\n",
      "Training Loss: 0.7213224768638611\n",
      "Validation Loss: 0.7219489216804504\n",
      "BATCH NUM: 545\n",
      "Training Loss: 0.7170010209083557\n",
      "Validation Loss: 0.7170791625976562\n",
      "BATCH NUM: 546\n",
      "Training Loss: 0.7230951189994812\n",
      "Validation Loss: 0.7222393155097961\n",
      "BATCH NUM: 547\n",
      "Training Loss: 0.7444679737091064\n",
      "Validation Loss: 0.7443464994430542\n",
      "BATCH NUM: 548\n",
      "Training Loss: 0.717536449432373\n",
      "Validation Loss: 0.7182055711746216\n",
      "BATCH NUM: 549\n",
      "Training Loss: 0.7248069047927856\n",
      "Validation Loss: 0.7248153686523438\n",
      "BATCH NUM: 550\n",
      "Training Loss: 0.7285758256912231\n",
      "Validation Loss: 0.7295735478401184\n",
      "BATCH NUM: 551\n",
      "Training Loss: 0.7440100312232971\n",
      "Validation Loss: 0.7439257502555847\n",
      "BATCH NUM: 552\n",
      "Training Loss: 0.7241503596305847\n",
      "Validation Loss: 0.7235679030418396\n",
      "BATCH NUM: 553\n",
      "Training Loss: 0.7220661640167236\n",
      "Validation Loss: 0.722578227519989\n",
      "BATCH NUM: 554\n",
      "Training Loss: 0.7362407445907593\n",
      "Validation Loss: 0.7373279333114624\n",
      "BATCH NUM: 555\n",
      "Training Loss: 0.7270554900169373\n",
      "Validation Loss: 0.7271194458007812\n",
      "BATCH NUM: 556\n",
      "Training Loss: 0.7232598662376404\n",
      "Validation Loss: 0.7225457429885864\n",
      "BATCH NUM: 557\n",
      "Training Loss: 0.7292656898498535\n",
      "Validation Loss: 0.72942054271698\n",
      "BATCH NUM: 558\n",
      "Training Loss: 0.7189490795135498\n",
      "Validation Loss: 0.7193631529808044\n",
      "BATCH NUM: 559\n",
      "Training Loss: 0.7067117094993591\n",
      "Validation Loss: 0.7064917087554932\n",
      "BATCH NUM: 560\n",
      "Training Loss: 0.730837881565094\n",
      "Validation Loss: 0.7305970788002014\n",
      "BATCH NUM: 561\n",
      "Training Loss: 0.7169035077095032\n",
      "Validation Loss: 0.7158351540565491\n",
      "BATCH NUM: 562\n",
      "Training Loss: 0.7432096004486084\n",
      "Validation Loss: 0.7431105375289917\n",
      "BATCH NUM: 563\n",
      "Training Loss: 0.7288428544998169\n",
      "Validation Loss: 0.7283608913421631\n",
      "BATCH NUM: 564\n",
      "Training Loss: 0.7277155518531799\n",
      "Validation Loss: 0.7276986241340637\n",
      "BATCH NUM: 565\n",
      "Training Loss: 0.7188353538513184\n",
      "Validation Loss: 0.7191951274871826\n",
      "BATCH NUM: 566\n",
      "Training Loss: 0.719236433506012\n",
      "Validation Loss: 0.7199481129646301\n",
      "BATCH NUM: 567\n",
      "Training Loss: 0.7128462791442871\n",
      "Validation Loss: 0.7127935290336609\n",
      "BATCH NUM: 568\n",
      "Training Loss: 0.7352479100227356\n",
      "Validation Loss: 0.7352076172828674\n",
      "BATCH NUM: 569\n",
      "Training Loss: 0.7228747606277466\n",
      "Validation Loss: 0.7226806879043579\n",
      "BATCH NUM: 570\n",
      "Training Loss: 0.7394381165504456\n",
      "Validation Loss: 0.7390616536140442\n",
      "BATCH NUM: 571\n",
      "Training Loss: 0.7298555374145508\n",
      "Validation Loss: 0.7291620373725891\n",
      "BATCH NUM: 572\n",
      "Training Loss: 0.7311457991600037\n",
      "Validation Loss: 0.7312333583831787\n",
      "BATCH NUM: 573\n",
      "Training Loss: 0.7300233244895935\n",
      "Validation Loss: 0.7306406497955322\n",
      "BATCH NUM: 574\n",
      "Training Loss: 0.7454010844230652\n",
      "Validation Loss: 0.7450855374336243\n",
      "BATCH NUM: 575\n",
      "Training Loss: 0.725565493106842\n",
      "Validation Loss: 0.7251843214035034\n",
      "BATCH NUM: 576\n",
      "Training Loss: 0.7395209670066833\n",
      "Validation Loss: 0.7394312024116516\n",
      "BATCH NUM: 577\n",
      "Training Loss: 0.7192773222923279\n",
      "Validation Loss: 0.7200987339019775\n",
      "BATCH NUM: 578\n",
      "Training Loss: 0.7161726355552673\n",
      "Validation Loss: 0.7164765000343323\n",
      "BATCH NUM: 579\n",
      "Training Loss: 0.7269599437713623\n",
      "Validation Loss: 0.7270947694778442\n",
      "BATCH NUM: 580\n",
      "Training Loss: 0.7192500233650208\n",
      "Validation Loss: 0.7194139361381531\n",
      "BATCH NUM: 581\n",
      "Training Loss: 0.7190088629722595\n",
      "Validation Loss: 0.7195531725883484\n",
      "BATCH NUM: 582\n",
      "Training Loss: 0.7373497486114502\n",
      "Validation Loss: 0.7371852397918701\n",
      "BATCH NUM: 583\n",
      "Training Loss: 0.7193661332130432\n",
      "Validation Loss: 0.7191836833953857\n",
      "BATCH NUM: 584\n",
      "Training Loss: 0.7481726408004761\n",
      "Validation Loss: 0.7469711303710938\n",
      "BATCH NUM: 585\n",
      "Training Loss: 0.7224273085594177\n",
      "Validation Loss: 0.7217668890953064\n",
      "BATCH NUM: 586\n",
      "Training Loss: 0.7292876839637756\n",
      "Validation Loss: 0.7288005352020264\n",
      "BATCH NUM: 587\n",
      "Training Loss: 0.7374628186225891\n",
      "Validation Loss: 0.7372511625289917\n",
      "BATCH NUM: 588\n",
      "Training Loss: 0.7307431101799011\n",
      "Validation Loss: 0.7304844260215759\n",
      "BATCH NUM: 589\n",
      "Training Loss: 0.7128498554229736\n",
      "Validation Loss: 0.7132229208946228\n",
      "BATCH NUM: 590\n",
      "Training Loss: 0.726920485496521\n",
      "Validation Loss: 0.7273929119110107\n",
      "BATCH NUM: 591\n",
      "Training Loss: 0.714288592338562\n",
      "Validation Loss: 0.7153749465942383\n",
      "BATCH NUM: 592\n",
      "Training Loss: 0.7182410359382629\n",
      "Validation Loss: 0.7190698981285095\n",
      "BATCH NUM: 593\n",
      "Training Loss: 0.738567054271698\n",
      "Validation Loss: 0.7382959127426147\n",
      "BATCH NUM: 594\n",
      "Training Loss: 0.7187497615814209\n",
      "Validation Loss: 0.7185544967651367\n",
      "BATCH NUM: 595\n",
      "Training Loss: 0.7042464017868042\n",
      "Validation Loss: 0.7045385241508484\n",
      "BATCH NUM: 596\n",
      "Training Loss: 0.7221810817718506\n",
      "Validation Loss: 0.7223272323608398\n",
      "BATCH NUM: 597\n",
      "Training Loss: 0.7302197813987732\n",
      "Validation Loss: 0.7295505404472351\n",
      "BATCH NUM: 598\n",
      "Training Loss: 0.7336189150810242\n",
      "Validation Loss: 0.7338230609893799\n",
      "BATCH NUM: 599\n",
      "Training Loss: 0.7260499596595764\n",
      "Validation Loss: 0.7260570526123047\n",
      "BATCH NUM: 600\n",
      "Training Loss: 0.7236423492431641\n",
      "Validation Loss: 0.7232645750045776\n",
      "BATCH NUM: 601\n",
      "Training Loss: 0.7293218374252319\n",
      "Validation Loss: 0.7281290292739868\n",
      "BATCH NUM: 602\n",
      "Training Loss: 0.7137120962142944\n",
      "Validation Loss: 0.713147759437561\n",
      "BATCH NUM: 603\n",
      "Training Loss: 0.726529061794281\n",
      "Validation Loss: 0.7268293499946594\n",
      "BATCH NUM: 604\n",
      "Training Loss: 0.7278380990028381\n",
      "Validation Loss: 0.727620005607605\n",
      "BATCH NUM: 605\n",
      "Training Loss: 0.7256661653518677\n",
      "Validation Loss: 0.726041316986084\n",
      "BATCH NUM: 606\n",
      "Training Loss: 0.725618839263916\n",
      "Validation Loss: 0.7244660258293152\n",
      "BATCH NUM: 607\n",
      "Training Loss: 0.7285895347595215\n",
      "Validation Loss: 0.7288945317268372\n",
      "BATCH NUM: 608\n",
      "Training Loss: 0.714826762676239\n",
      "Validation Loss: 0.7149227857589722\n",
      "BATCH NUM: 609\n",
      "Training Loss: 0.7190095782279968\n",
      "Validation Loss: 0.7195633053779602\n",
      "BATCH NUM: 610\n",
      "Training Loss: 0.695933997631073\n",
      "Validation Loss: 0.6968594789505005\n",
      "BATCH NUM: 611\n",
      "Training Loss: 0.7210696935653687\n",
      "Validation Loss: 0.7207488417625427\n",
      "BATCH NUM: 612\n",
      "Training Loss: 0.7419486045837402\n",
      "Validation Loss: 0.7409644722938538\n",
      "BATCH NUM: 613\n",
      "Training Loss: 0.7419267296791077\n",
      "Validation Loss: 0.7421279549598694\n",
      "BATCH NUM: 614\n",
      "Training Loss: 0.7309591174125671\n",
      "Validation Loss: 0.7307018041610718\n",
      "BATCH NUM: 615\n",
      "Training Loss: 0.7473264336585999\n",
      "Validation Loss: 0.7469495534896851\n",
      "BATCH NUM: 616\n",
      "Training Loss: 0.721631646156311\n",
      "Validation Loss: 0.7218853235244751\n",
      "BATCH NUM: 617\n",
      "Training Loss: 0.7373210787773132\n",
      "Validation Loss: 0.7379075288772583\n",
      "BATCH NUM: 618\n",
      "Training Loss: 0.7096856832504272\n",
      "Validation Loss: 0.7103445529937744\n",
      "BATCH NUM: 619\n",
      "Training Loss: 0.7150855660438538\n",
      "Validation Loss: 0.7149306535720825\n",
      "BATCH NUM: 620\n",
      "Training Loss: 0.7300426959991455\n",
      "Validation Loss: 0.7290663123130798\n",
      "BATCH NUM: 621\n",
      "Training Loss: 0.7153865098953247\n",
      "Validation Loss: 0.715035080909729\n",
      "BATCH NUM: 622\n",
      "Training Loss: 0.7458047270774841\n",
      "Validation Loss: 0.7448329925537109\n",
      "BATCH NUM: 623\n",
      "Training Loss: 0.7303199768066406\n",
      "Validation Loss: 0.7306033372879028\n",
      "BATCH NUM: 624\n",
      "Training Loss: 0.7344548106193542\n",
      "Validation Loss: 0.7340919375419617\n",
      "BATCH NUM: 625\n",
      "Training Loss: 0.7160398960113525\n",
      "Validation Loss: 0.7163447141647339\n",
      "BATCH NUM: 626\n",
      "Training Loss: 0.7243106365203857\n",
      "Validation Loss: 0.723722517490387\n",
      "BATCH NUM: 627\n",
      "Training Loss: 0.7143610119819641\n",
      "Validation Loss: 0.7143685221672058\n",
      "BATCH NUM: 628\n",
      "Training Loss: 0.7515882253646851\n",
      "Validation Loss: 0.7515996098518372\n",
      "BATCH NUM: 629\n",
      "Training Loss: 0.7179778814315796\n",
      "Validation Loss: 0.7183706164360046\n",
      "BATCH NUM: 630\n",
      "Training Loss: 0.7311936616897583\n",
      "Validation Loss: 0.7321395874023438\n",
      "BATCH NUM: 631\n",
      "Training Loss: 0.7317373752593994\n",
      "Validation Loss: 0.7316306233406067\n",
      "BATCH NUM: 632\n",
      "Training Loss: 0.7445907592773438\n",
      "Validation Loss: 0.7435926795005798\n",
      "BATCH NUM: 633\n",
      "Training Loss: 0.7350161671638489\n",
      "Validation Loss: 0.7352229952812195\n",
      "BATCH NUM: 634\n",
      "Training Loss: 0.743617594242096\n",
      "Validation Loss: 0.7435183525085449\n",
      "BATCH NUM: 635\n",
      "Training Loss: 0.7287415266036987\n",
      "Validation Loss: 0.7281797528266907\n",
      "BATCH NUM: 636\n",
      "Training Loss: 0.750372588634491\n",
      "Validation Loss: 0.7507503032684326\n",
      "BATCH NUM: 637\n",
      "Training Loss: 0.7089262008666992\n",
      "Validation Loss: 0.7080771923065186\n",
      "BATCH NUM: 638\n",
      "Training Loss: 0.7314884066581726\n",
      "Validation Loss: 0.7318836450576782\n",
      "BATCH NUM: 639\n",
      "Training Loss: 0.7285635471343994\n",
      "Validation Loss: 0.7280240654945374\n",
      "BATCH NUM: 640\n",
      "Training Loss: 0.7299249768257141\n",
      "Validation Loss: 0.7296484112739563\n",
      "BATCH NUM: 641\n",
      "Training Loss: 0.7050551176071167\n",
      "Validation Loss: 0.7050153017044067\n",
      "BATCH NUM: 642\n",
      "Training Loss: 0.7233375310897827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m vad_model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Переводим модель в режим валидации\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Отключаем градиенты для ускорения\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvad_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Передаем реальный батч\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     12\u001b[0m batch_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[20], line 49\u001b[0m, in \u001b[0;36mVADTESTING.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m     48\u001b[0m     x, t \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspectro\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     50\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(probs, t)\n\u001b[0;32m     52\u001b[0m     probs \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 25\u001b[0m, in \u001b[0;36mVADTESTING.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 25\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probs\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Diploma\\VD-KazakhSpeech\\models.py:53\u001b[0m, in \u001b[0;36mVADNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 53\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mflatten(x, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, end_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Diploma\\VD-KazakhSpeech\\models.py:30\u001b[0m, in \u001b[0;36mCNNEmbedder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvblock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvblock2(x)\n\u001b[0;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvblock3(x)\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Diploma\\VD-KazakhSpeech\\models.py:17\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\b.smadiarov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_num = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(test_train_loader):\n",
    "    vad_model.train()  # Переводим модель в режим тренировки\n",
    "    training_loss = vad_model.training_step(batch, batch_idx=batch_idx)  # Передаем реальный батч\n",
    "    print(f'BATCH NUM: {batch_num}')\n",
    "    print(\"Training Loss:\", training_loss.item())\n",
    "    vad_model.eval()  # Переводим модель в режим валидации\n",
    "    with th.no_grad():  # Отключаем градиенты для ускорения\n",
    "        val_loss = vad_model.validation_step(batch, batch_idx=batch_idx)  # Передаем реальный батч\n",
    "    print(\"Validation Loss:\", val_loss.item())\n",
    "    batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854397d-335e-4116-b094-d4d34bad71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ccaebf-2b33-4757-a750-89dedb8a153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8688, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.8688, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output_i = loss(input, target)\n",
    "print(output_i)\n",
    "output_i.backward()\n",
    "print(output_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ff5d2a-c632-4ab9-a01d-ab27ffaa442f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pytorch_lightning' has no attribute 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241m.\u001b[39mclassification\u001b[38;5;241m.\u001b[39mF1()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pytorch_lightning' has no attribute 'metrics'"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "f1 = pl.metrics.classification.F1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351d9f7-0a59-4456-9884-75a063a78be6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Torch training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1eb836a-edc3-4b2e-b857-4f1a9859bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def p_output_log(epoch, num_epochs, phase, epoch_loss, epoch_metrics, metrics):\n",
    "    if phase == 'train':\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f\"{phase.upper()}, Loss: {epoch_loss:.4f}, \", end=\"\")\n",
    "    for m in metrics.keys():\n",
    "        print(f\"{m}: {epoch_metrics[m]:.4f} \", end=\"\")\n",
    "    print() \n",
    "    if phase == 'valid':\n",
    "        print('-' * 108, '\\n')\n",
    "\n",
    "def __train_model(model, dataloaders, criterion, optimizer, metrics, num_epochs=25, device='cuda'):\n",
    "    model.to(device)\n",
    "    min_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = dataloaders['train']\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = dataloaders['valid']\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            [metrics[m].reset() for m in metrics.keys()]\n",
    "            total_samples = len(dataloader.dataset)\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs).transpose(1, 2)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # Так как значение loss.item() уже усреднено внутри батча, \n",
    "                # чтобы получить общую сумму потерь (а не среднюю) для этого батча, \n",
    "                # нужно домножить её на количество объектов в батче, то есть на inputs.size(0).\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                [metrics[m].update(outputs, labels) for m in metrics.keys()]\n",
    "            \n",
    "            epoch_loss = running_loss / total_samples\n",
    "            epoch_metrics = {m: metrics[m].compute().item() for m in metrics.keys()}\n",
    "            \n",
    "            p_output_log(epoch, num_epochs, phase, epoch_loss, epoch_metrics, metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05534755-1407-453f-b1b5-9dde6fb155ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 18349\n",
      "Size of validation set: 2039\n",
      "Trainable parametrs: 621477\n",
      "Epoch 1/100\n",
      "TRAIN, Loss: 0.9608, Accuracy: 0.5072 \n",
      "VALID, Loss: 0.7164, Accuracy: 0.5277 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 2/100\n",
      "TRAIN, Loss: 0.6949, Accuracy: 0.5225 \n",
      "VALID, Loss: 0.6855, Accuracy: 0.5065 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 3/100\n",
      "TRAIN, Loss: 0.5706, Accuracy: 0.6956 \n",
      "VALID, Loss: 0.6047, Accuracy: 0.6911 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 4/100\n",
      "TRAIN, Loss: 0.2384, Accuracy: 0.9092 \n",
      "VALID, Loss: 0.3443, Accuracy: 0.8599 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 5/100\n",
      "TRAIN, Loss: 0.1986, Accuracy: 0.9252 \n",
      "VALID, Loss: 0.5876, Accuracy: 0.7743 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 6/100\n",
      "TRAIN, Loss: 0.1607, Accuracy: 0.9404 \n",
      "VALID, Loss: 0.5837, Accuracy: 0.7961 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 7/100\n",
      "TRAIN, Loss: 0.1417, Accuracy: 0.9498 \n",
      "VALID, Loss: 0.1463, Accuracy: 0.9525 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 8/100\n",
      "TRAIN, Loss: 0.1217, Accuracy: 0.9565 \n",
      "VALID, Loss: 0.1926, Accuracy: 0.9271 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 9/100\n",
      "TRAIN, Loss: 0.1092, Accuracy: 0.9615 \n",
      "VALID, Loss: 0.3185, Accuracy: 0.8939 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 10/100\n",
      "TRAIN, Loss: 0.1101, Accuracy: 0.9605 \n",
      "VALID, Loss: 0.1878, Accuracy: 0.9245 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 11/100\n",
      "TRAIN, Loss: 0.1051, Accuracy: 0.9614 \n",
      "VALID, Loss: 0.1386, Accuracy: 0.9496 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 12/100\n",
      "TRAIN, Loss: 0.1124, Accuracy: 0.9583 \n",
      "VALID, Loss: 0.2051, Accuracy: 0.9137 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 13/100\n",
      "TRAIN, Loss: 0.1060, Accuracy: 0.9593 \n",
      "VALID, Loss: 0.1708, Accuracy: 0.9344 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 14/100\n",
      "TRAIN, Loss: 0.1168, Accuracy: 0.9558 \n",
      "VALID, Loss: 0.5390, Accuracy: 0.7914 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 15/100\n",
      "TRAIN, Loss: 0.1156, Accuracy: 0.9565 \n",
      "VALID, Loss: 0.2722, Accuracy: 0.9015 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 16/100\n",
      "TRAIN, Loss: 0.1385, Accuracy: 0.9473 \n",
      "VALID, Loss: 0.3022, Accuracy: 0.8559 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 17/100\n",
      "TRAIN, Loss: 0.1767, Accuracy: 0.9344 \n",
      "VALID, Loss: 0.2308, Accuracy: 0.9111 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 18/100\n",
      "TRAIN, Loss: 0.2032, Accuracy: 0.9232 \n",
      "VALID, Loss: 0.3428, Accuracy: 0.8678 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 19/100\n",
      "TRAIN, Loss: 0.1650, Accuracy: 0.9391 \n",
      "VALID, Loss: 0.3852, Accuracy: 0.8297 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 20/100\n",
      "TRAIN, Loss: 0.2086, Accuracy: 0.9186 \n",
      "VALID, Loss: 0.7287, Accuracy: 0.8013 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 21/100\n",
      "TRAIN, Loss: 0.1914, Accuracy: 0.9276 \n",
      "VALID, Loss: 0.3044, Accuracy: 0.8779 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 22/100\n",
      "TRAIN, Loss: 0.2612, Accuracy: 0.8879 \n",
      "VALID, Loss: 0.6221, Accuracy: 0.7324 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 23/100\n",
      "TRAIN, Loss: 0.4689, Accuracy: 0.7813 \n",
      "VALID, Loss: 0.3637, Accuracy: 0.8473 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 24/100\n",
      "TRAIN, Loss: 0.3551, Accuracy: 0.8497 \n",
      "VALID, Loss: 0.4951, Accuracy: 0.7855 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 25/100\n",
      "TRAIN, Loss: 0.3521, Accuracy: 0.8437 \n",
      "VALID, Loss: 0.3045, Accuracy: 0.8677 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 26/100\n",
      "TRAIN, Loss: 0.2901, Accuracy: 0.8797 \n",
      "VALID, Loss: 0.4814, Accuracy: 0.7909 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 27/100\n",
      "TRAIN, Loss: 0.3447, Accuracy: 0.8581 \n",
      "VALID, Loss: 0.3361, Accuracy: 0.8678 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 28/100\n",
      "TRAIN, Loss: 0.2834, Accuracy: 0.8964 \n",
      "VALID, Loss: 0.3706, Accuracy: 0.8454 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 29/100\n",
      "TRAIN, Loss: 0.3285, Accuracy: 0.8671 \n",
      "VALID, Loss: 0.3374, Accuracy: 0.8577 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 30/100\n",
      "TRAIN, Loss: 0.3667, Accuracy: 0.8366 \n",
      "VALID, Loss: 0.6021, Accuracy: 0.7291 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 31/100\n",
      "TRAIN, Loss: 0.3832, Accuracy: 0.8364 \n",
      "VALID, Loss: 0.3929, Accuracy: 0.8235 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 32/100\n",
      "TRAIN, Loss: 0.4381, Accuracy: 0.7942 \n",
      "VALID, Loss: 0.4037, Accuracy: 0.8178 \n",
      "------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch 33/100\n",
      "TRAIN, Loss: 0.3403, Accuracy: 0.8580 \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "# from torch_trainer import train_model\n",
    "import yaml\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import omegaconf as om\n",
    "\n",
    "from models import VADNet \n",
    "from torching_datasets import *\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def main():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    cfg = {\n",
    "        'data': {\n",
    "            'data_dir': 'F:/ISSAI_KSC2_unpacked/temp_vad',\n",
    "            'batch_size': 512,\n",
    "            'valid_percent': 0.9,\n",
    "            'n_frames': 32,\n",
    "            'nfft': 1048,\n",
    "            'hop_length': 512,\n",
    "            'n_mels': 128,\n",
    "            'sr': 16000,\n",
    "            'norm': False,\n",
    "            'n_workers': 2,\n",
    "            'pin_memory': True,\n",
    "            'seed': 42\n",
    "        },\n",
    "        'model': {\n",
    "            'n_feat': 128,\n",
    "            'cnn_channels': 32,\n",
    "            'embed_dim': 256,\n",
    "            'dff': 512,\n",
    "            'num_heads': 16\n",
    "        },\n",
    "        'training': {\n",
    "            'optim': 'Adam',\n",
    "            'lr': 0.01,\n",
    "            'weight_decay': 1e-05\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Load data \n",
    "    datamodule = VADMelDataModule(**cfg['data'])\n",
    "    datamodule.setup()\n",
    "    dataloaders = {'train': datamodule.train_dataloader(), 'valid': datamodule.val_dataloader()}\n",
    "\n",
    "    model = VADNet(**cfg['model'])\n",
    "    \n",
    "    # Meta-data\n",
    "    print(f\"Trainable parametrs: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    # Optimizer\n",
    "    optim_type = cfg['training'][\"optim\"]\n",
    "    assert  optim_type in ['Adam', 'SGD']\n",
    "    if optim_type == 'Adam':\n",
    "        optimizer = th.optim.Adam(model.parameters(), lr=cfg['training'][\"lr\"], weight_decay=cfg['training'][\"weight_decay\"])\n",
    "    else: \n",
    "        optimizer = th.optim.SGD(model.parameters(), lr=cfg['training'][\"lr\"], weight_decay=cfg['training'][\"weight_decay\"])\n",
    "\n",
    "    # Metrics: accuracy. Changes macro changed to micro\n",
    "    metrics = {m: getattr(torchmetrics, m)(task='binary', average='micro').to(device) for m in ['Accuracy']}\n",
    "\n",
    "    # Start training. Hardcode: num_epochs = 100\n",
    "    trained_model = __train_model(model, dataloaders, nn.BCEWithLogitsLoss(), optimizer, metrics, num_epochs=100, \n",
    "                                device=device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35035fd3-d1e9-4e08-a218-27e33355d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # Пример входного тензора (input size)\n",
    "# input_tensor = torch.randn(512, 32)  # Тензор размера [512, 32]\n",
    "# # Пример целевого тензора (target size)\n",
    "# target_tensor = torch.randn(512, 1, 32)  # Тензор размера [512, 1, 32]\n",
    "# # Печать форм тензоров\n",
    "# print(\"Input tensor size:\", input_tensor.size())\n",
    "# print(\"Target tensor size:\", target_tensor.size())\n",
    "\n",
    "# a_target_tensor = target_tensor.squeeze(1) # squeeze -1 transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7dbe77-b43d-409f-85c4-692cd20b8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = 0.00001\n",
    "b = 1e-05\n",
    "print(a == b )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668db464-179f-4e79-be90-f990f0d46ae4",
   "metadata": {},
   "source": [
    "#### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f759cd-86f3-4246-94b3-76ccfb7cf7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Функция для построения ROC и PR-кривых\n",
    "def plot_roc_pr_curves(y_true, y_scores):\n",
    "    # ROC-кривая\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # PR-кривая\n",
    "    precision, recall, thresholds_pr = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    # Построение графиков\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # ROC-кривая\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\", color=\"blue\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random classifier\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # PR-кривая\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, label=f\"PR curve (AUC = {pr_auc:.2f})\", color=\"green\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Пример использования\n",
    "# y_true - истинные метки классов (0 или 1)\n",
    "# y_scores - вероятности от модели\n",
    "y_true = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 0])  # Истинные метки\n",
    "y_scores = np.array([0.1, 0.9, 0.8, 0.3, 0.7, 0.2, 0.95, 0.85, 0.4, 0.05])  # Предсказанные вероятности\n",
    "\n",
    "plot_roc_pr_curves(y_true, y_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c2499-e1fe-46b2-bebd-30bc1d9438e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предположим, что у вас есть список аудиофайлов и их истинные метки\n",
    "audio_files = [\"file1.wav\", \"file2.wav\", \"file3.wav\", ...]\n",
    "true_labels = [0, 1, 0, 1, ...]  # Истинные метки\n",
    "\n",
    "# Собираем предсказания\n",
    "predicted_probs = []\n",
    "for audio_path in audio_files:\n",
    "    probs = your_model.predict(audio_path)  # Используем вашу функцию predict\n",
    "    predicted_probs.append(probs.mean())   # Например, усредняем вероятности по всем временным меткам\n",
    "\n",
    "# Преобразуем списки в массивы\n",
    "y_true = np.array(true_labels)\n",
    "y_scores = np.array(predicted_probs)\n",
    "\n",
    "# Строим ROC и PR кривые\n",
    "plot_roc_pr_curves(y_true, y_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332b040-995f-466c-8259-51dafb86b134",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b68540-f469-46f2-9a31-e26dfa006ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a16f0-759b-43b2-b453-9d3a1aa728c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
